---
title: "Reproducible Research and GIScience: an evaluation using GIScience conference papers"
titlerunning: "Reproducible GIScience"
format: "a4paper"
hyphenation: "UKenglish"
authorcolumns: false
numberwithinsect: false # for section-numbered lemmas etc.
cleveref: true # for enabling cleveref support
autoref: true # for enabling autoref support
anonymous: false # for anonymousing the authors (e.g. for double-blind review)
thm-restate: true # for enabling thm-restate support
author:
  # mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional
  - name: Barbara Hofer
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Interfaculty Department of Geoinformatics - Z_GIS, University of Salzburg, Salzburg, Austria"
    orcid: "https://orcid.org/0000-0001-7078-3766"
    email: barbara.hofer@sbg.ac.at
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Carlos Granell
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute of New Imaging Technologies, Universitat Jaume I de Castellón, Castellón, Spain"
    orcid: "https://orcid.org/0000-0003-1004-9695"
    email: carlos.granell@uji.es
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Daniel Nüst
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0002-0024-5046"
    email: daniel.nuest@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant number [PE&nbsp;1632/17-1](https://gepris.dfg.de/gepris/projekt/415851837)."
  - name: Frank Ostermann
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, The Netherlands"
    orcid: "https://orcid.org/0000-0002-9317-8291"
    email: f.o.ostermann@utwente.nl
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Markus Konkol
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0001-6651-0976"
    email: m.konkol@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant number [KR&nbsp;3930/8-1](https://gepris.dfg.de/gepris/projekt/415851837)."
abstract: |
  Authors of papers at the GIScience conference face challenges of computational reproducibility just like any other discipline using computers to analyse data.
  In this work, we apply a rubric for assessing the reproducibility to XX conference papers published at the GIScience conference series in years 20XX-2018.
  The rubric was originally developed for an assessment of publications at the AGILE conference.
  The results of the GIScience paper assessment are in line with previous findings.
  The description of workflows and publication of data and software used in most papers suffice to explain the presented work, but are far from enabling a reproduction by a third party with reasonable effort.
  We summarise and adapt previous recommendations for improving this dire picture and invite the GIScience community to start a broad discussion on reusability, quality, and openness of its research.
bibliography: bibliography
authorrunning: "TODO authorrunning when order fixed" # mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'
# A "thin space" character, ' ' or &thinsp;, is used between the two first names.
copyright: "TODO copyright when order fixed, John Q. Public and Joan R. Public" # mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/
ccdesc:
  # Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
  concept_significance: "500"
  concept_desc: "Information systems~Geographic information systems"
keywords: "reproducible research, open science, reproducibility, GIScience"
# OPTIONAL:
#acknowledgements: "I want to thank \\dots"
#category: "Invited paper"
#relatedversion: "A full version of the paper is available at https://..." # optional, e.g. full version hosted on arXiv, HAL, or other respository/website
# optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
supplement: |
  The raw data for this work are the full texts of conference GIScience conference proceedings \cite{giscienceproceedings2018,giscienceproceedings2016}.
  The paper assessment results and source code of figures are published at [https://github.com/nuest/reproducible-research-at-giscience](github.com/nuest/reproducible-research-at-giscience) and archived at TODO ZENODO URL.
  The used computing environment is [containerised with Docker and Binder-ready using R&nbsp;3.6.0](https://github.com/rocker-org/binder/) and an [MRAN snapshot](https://mran.microsoft.com/timemachine) of July 5th 2019.
nolinenumbers: false # disable line numbering
hideLIPIcs: false # remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository
# appendix _after_ the bibliography
#appendix: |
output:
  # xelatex needed for 'thin space' Unicode character (see authorrunning);
  bookdown::pdf_book:
    base_format: rticles::lipics_article
    latex_engine: xelatex
    keep_tex: TRUE
---

# Introduction

A large proportion of GIScience research today uses software to analyse data on computers.
This makes a considerable share of the articles published in the context of the GIScience conference series[^giscience] fall into the categories of data science or computational research.
Thereby these articles are exposed to challenges of transparency and reproducibility in the sense of the Claerbout/Donoho/Peng terminology \cite{barba_terminologies_2018}, where reproduction means a recreation of the same results using the same input data and workflow as the original authors.
In previous work \cite{nust_reproducible_2018} we assessed the reproducibility of full and short papers of the AGILE conference series[^agile], a community conference closely related to GIScience. 
Using a rubric for reproducible research, we found that the majority of articles did not provide sufficient information for a reviewer to evaluate code and data and attempt a reproduction, and neither enough material for readers to reuse or extend the workflows.
This is corroborated by research in related disciplines such as quantitative geography \cite{brunsdon_quantitative_2016}, qualitative GIS \cite{muenchow_reviewing_2019}, geoscience \cite{konkol_computational_2018}, and computer science [TODO REF].
The problems identified in these related research areas are directly transferable to GIScience, which operates at the intersections of aforementioned fields \cite{Goodchild1992}.
In any case, observations on lack of reproducibility in all scientific fields are contrasted by clear advantages and benefits  of open and reproducible research both for individuals and for academia as a whole (cf. for example \cite{donoho_invitation_2010,markowetz_five_2015,kray_reproducible_2019,Colavizza2020}).

We have conducted a simple text analysis of the proceedings[^textanalysis] to evaluate the relevance of computational methods in GIScience conference papers.
The analysis finds wordstems related to reproducibility: generic words, e.g., "data", "software", or "process", specific platforms, e.g., "GitHub", and concrete terms, e.g., words starting with "reproduc".
The take away message from the text analysis that algorithms, processing, and data play a large role in GIScience publications, but only little mentioning of code repositories or reproduction material could be identified.
Therefore, a more detailed manual assessment of the reproducibility of these publications is necessary.
The AGILE and GIScience conference series are conference series that share a common discipline, and have significant overlap in audience.
<!-- TODO: quantify overlap -->
Nevertheless, based on our own experience with these two events, we postulate that there are also significant differences in terms of topic focus and methods applied, that preclude the simplistic assumption that the findings for AGILE apply to GIScience as well, and instead warrant an investigation. 

The main contribution of this work addresses two objectives:
First, to apply the assessment procedure used for AGILE conference papers (presented in the next section) to the papers of the GIScience conference, to check whether it is generic enough to allow replication with a different dataset.
This transfer solidifies the methodology and yields important findings for the discussion of reproducibility within the GIScience conference community and the broader GIScience discipline at large. Second, to broaden our knowledge base about reproducibility in GIScience in general, and learn more about the situation in the GIScience conference series.
Only then can a fruitful discussion take place on ways to improve reproducibility (if necessary) for the GIScience conference series, and whether the recent steps taken at AGILE \cite{reproducible_agile} could be an inspiration for GIScience conferences as well.
We discuss these findings and present our conclusions in the final two sections.

[^giscience]: [https://www.giscience.org/](https://www.giscience.org/)
[^agile]: [https://agile-online.org/conference](https://agile-online.org/conference)
[^textanalysis]: The full text analysis results is available in this paper's repository: [`giscience-historic-text-analysis.Rmd`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-historic-text-analysis.Rmd) contains the analysis code and the result files are [`text_analysis_topwordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_topwordstems.csv) and [`text_analysis_keywordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_keywordstems.csv).

# Reproducibility Assessment

## Criteria

The overall approach to assessing the reproducibility of GIScience papers is similar to the assessment of AGILE papers \cite{nust_reproducible_2018}:
Two members of the author team reviewed each GIScience paper along three main criteria, assigning one of four reproducibility levels to each criterion.
The three assessment criteria are _Input Data_, _Methods_, and _Results_. 
_Input Data_ comprises all datasets that the computational analysis uses.
_Methods_ encompasses the entire computational analysis that generates the results.
Since _Methods_ is difficult to evaluate as a whole, we split this criterion into three subcategories:
_Preprocessing_ includes the steps to prepare the _Input Data_ before main analysis;
_Methods, Analysis, Processing_ is the actual analysis; 
_Computational Environment_ addresses the description of the hard- and software.
Finally, the criterion _Results_ refers to the output of analysis, for example, figures, tables, and numbers. 
For each of these (sub)categories, we assigned one of four levels:
_(Level 0) Unavailable_ means that it is not possible to access the paper’s data, methods, or results, and that it is impossible to recreate them based on the description in the paper.
_(Level 1) Documented_ indicates that the paper still does not provide access to datasets, methods, or results, but that there is sufficient description to recreate them closely enough for an evaluation, yet often a recreation is unlikely due to huge efforts needed.
_(Level 2) Available_ is assigned if the paper provides direct access to the materials (e.g., through a personal or institutional website), but not in the form of an open and permanent identifier, such as a DOI. 
The gold standard, _(Level 3) Available and open_, requires open and permanent access to the materials (e.g., through public online repositories).

## Paper Corpus

```{r load_data, echo=FALSE, message=FALSE, warning=FALSE}
library("here")

assessment_file <- here::here("results/paper_assessment.csv")

category_levels <- c("0", "1", "2", "3")
categoryColumns <- c("input data", 
                     "preprocessing",
                     "method/analysis/processing",
                     "computational environment",
                     "results")

paper_evaluation <- readr::read_csv(assessment_file, 
    col_types = readr::cols(
      `conceptual paper` = readr::col_logical(),
      `computational environment` = readr::col_factor(levels = category_levels),
      `input data` = readr::col_factor(levels = category_levels),
      `method/analysis/processing` = readr::col_factor(levels = category_levels),
      preprocessing = readr::col_factor(levels = category_levels),
      results = readr::col_factor(levels = category_levels)
      ),
    na = "NA")
```

```{r count_papers, echo=FALSE, message=FALSE, warning=FALSE}
library("dplyr")

count_conceptual <- nrow(paper_evaluation %>% 
                           dplyr::filter(`conceptual paper` == TRUE))
count_mixed <- nrow(paper_evaluation %>% 
                      dplyr::filter(is.na(`input data`) 
                             | is.na(preprocessing) 
                             | is.na(`method/analysis/processing`) 
                             | is.na(`computational environment`) 
                             | is.na(results)))
```

## Process

Process similar to AGILE etc.

- Since there is no preselection, we looked at all GIScience papers, starting at 2018 and then iterating backwards until 20XX
- 5 assessors, all of which participated in the AGILE assessment
- We seem to have gotten stricter and applied “0” in cases where we previously might have used “NA” > reevaluation of the preprocessing category resulted in a few 0s being switched to NAs instead
- Should we write down how we _now_ understand _pre_processing?
- In “input data”, we clarified that “available” must have been the case once, but might not be now, e.g., if the given URL of an official site has changed
- Anecdotally, the papers seem to have more formulas
- How did we handle human subject experiments > needed to update the common understanding of the categories
- Note: this work is qualitative, and therefore not reproducible, so we try to be as transparent as possible

## Results

- `r nrow(paper_evaluation)` papers from the conferences in `r paste(sort(unique(paper_evaluation$year)), collapse = ", ")` were assessed.
- `r count_conceptual` papers across all years were identified as conceptual papers[^conceptual] and were not included in the corpus.
AGILE had 5 conceptual out of 32.
- `r paper_evaluation %>% filter(is.na(preprocessing)) %>% count() %>% .$n` papers are not applicable for preprocessing criterion.
- Table&nbsp;\@ref(tab:summary-evaldata) shows aggregated values for the assessed reproducibility levels.
- Figure&nbsp;\@ref(fig:assessment-results) shows the distribution of the reproducibility levels for each criterion.
- No `0` for results shows that peer review catches these papers.
- The full assessment table in the paper repository is at [`paper_assessment.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/paper_assessment.csv) 

```{r summary-evaldata, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
library("knitr")
library("kableExtra")

evaldata_numeric <- paper_evaluation %>%
  # must convert factors to numbers to calculate the mean and median
  dplyr::mutate_if(is.factor, dplyr::funs(as.integer(as.character(.))))

# apply summary independently to format as table
summaries <- sapply(evaldata_numeric[,categoryColumns], summary)
exclude_values_summary <- c("1st Qu.", "3rd Qu.")
kable(subset(summaries, !(rownames(summaries) %in% exclude_values_summary)), 
      digits = 1,
      col.names = c("input data", "preproc.", "method/analysis/proc.",
                    "comp. env.", "results"),
      caption = "Statistics of reproducibility levels per criterion (rounded to one decimal place)")
```

```{r assessment-results, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%', fig.cap="Barplots of reproducibility assessment results; levels range from 0 (leftmost bar) to 'not applicable' (rightmost bar)."}
# match the colours to time series plot below
colours <- RColorBrewer::brewer.pal(length(categoryColumns), "Set1")
level_names <- c("0", "1", "2", "3", "NA")

criteriaBarplot = function(data, main, colour) {
  barplot(table(data, useNA = "always"), 
          main = main,
          xlab = "Level",
          ylim = c(0,nrow(paper_evaluation)),
          names.arg = level_names, col = colours[colour])
}

par(mfrow = c(1,length(categoryColumns)),
    cex = 0.5,
    cex.axis = 0.9)
criteriaBarplot(paper_evaluation$`input data`,
                main = "Input data", colour = 1)
criteriaBarplot(paper_evaluation$`preprocessing`, 
                main = "Preprocessing", colour = 2)
criteriaBarplot(paper_evaluation$`method/analysis/processing`,
                main = "Methods/Analysis/\nProcessing", colour = 3)
criteriaBarplot(paper_evaluation$`computational environment`,
                main = "Computational\nEnvironment", colour = 4)
criteriaBarplot(paper_evaluation$results,
                main = "Results", colour = 5)
```

[^conceptual]: See \cite{nust_reproducible_2018} for a definition of "conceptual".

# Discussion

- Rubric worked, but faced similar challenges (category classes not equidistant!)
- Publication years similar to assessment of AGILE papers, so we think results are comparable in the sense of what methods and tools would have been available for authors.
- comparision to AGILE?

# Conclusions and outlook

In this work we investigated the reproducibility of several years of GIScience conference publications.
The paper corpus is large enough for a representative sample.
The corpus size is comparable to the one used for the AGILE study, but the corporas have different time windows.
It was never the intention of this study to rate the papers, or engage in a comparison of AGILE vs. GIScience conference quality.
We do not question that the research presented in these papers is sound and relevant, since they were accepted for publication at a reputable conference.
Instead, we investigated the papers along a single desirable quality dimension, reproducibility.
Assuming a similar high bar for reproducibility as in the earlier study, the results clearly show a lot of room for improvement, as none of the presented articles was readily reproducible.
The majority of articles provide some information, but not to the degree required to facilitate a change for more open, more transparent, and, most relevantly, reusable research.
In \cite{nust_reproducible_2018} we describe concrete recommendations for individuals and organisations to improve the reproducibility.
All these recommendations are directly transferable to the GIScience conference series, most importantly
(a) a promotion of outstanding reproducible work, e.g., with awards or badges,
(b) recognition of the efforts to achieve reproducibility, e.g., with a special track for reproducible papers, a role of reproducibility reviewer, open educational resources, and helpful author guidelines including data and software citation requirements and a specific data/software repository, and
(c) an institutional commitment to a shift in policies going beyond mere accessibility \cite{stodden_empirical_2018}.
These changes require a roadmap and clear year, say 2024, when GIScience starts to only accept computationally reproducible submissions and reproducibility is checked before acceptance of papers.
The AGILE reproducible paper guidelines \cite{agile_guidelines} and community code review systems such as CODECHECK \cite{eglen_codecheck_2019} are open and "ready to use", but also can be adopted for suitable reproducibility practices for the GIScience conferences.
The discourse around these recommendations would be beneficial for all members of the community, whether their work is more towards the conceptual, the computational, or the applied direction of GIScience.
An survey for authors, as conducted for AGILE \cite{nust_reproducible_2018}, can help to identify specific hindering circumstances for and special requirements of the GIScience conference community and is useful future work.

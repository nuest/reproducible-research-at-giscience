---
title: "Reproducible Research and GIScience: an evaluation using GIScience conference papers"
titlerunning: "Reproducible GIScience"
format: "a4paper"
hyphenation: "UKenglish"
authorcolumns: false
numberwithinsect: false # for section-numbered lemmas etc.
cleveref: true # for enabling cleveref support
autoref: true # for enabling autoref support
anonymous: false # for anonymousing the authors (e.g. for double-blind review)
thm-restate: true # for enabling thm-restate support
author:
  # mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional
  - name: Barbara Hofer
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Interfaculty Department of Geoinformatics - Z_GIS, University of Salzburg, Salzburg, Austria"
    orcid: "https://orcid.org/0000-0001-7078-3766"
    email: barbara.hofer@sbg.ac.at
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Carlos Granell
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute of New Imaging Technologies, Universitat Jaume I de Castellón, Castellón, Spain"
    orcid: "https://orcid.org/0000-0003-1004-9695"
    email: carlos.granell@uji.es
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Daniel Nüst
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0002-0024-5046"
    email: daniel.nuest@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant number [PE&nbsp;1632/17-1](https://gepris.dfg.de/gepris/projekt/415851837)."
  - name: Frank Ostermann
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, The Netherlands"
    orcid: "https://orcid.org/0000-0002-9317-8291"
    email: f.o.ostermann@utwente.nl
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Markus Konkol
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0001-6651-0976"
    email: m.konkol@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant numbers [KR&nbsp;3930/8-1](https://gepris.dfg.de/gepris/projekt/415851837) and [TR&nbsp;864/12-1]()."
abstract: |
  Authors of papers at the GIScience conference face challenges of computational reproducibility just like authors of any other discipline using computers to analyse data.
  In this work, we apply a rubric for assessing the reproducibility of 74 conference papers published at the GIScience conference series in years 2012-2018.
  The rubric and process were previously applied to the publications of the AGILE conference series.
  The results of the GIScience paper assessment are in line with previous findings:
  The description of workflows and publication of data and software used in most papers suffice to explain the presented work, but are far from enabling a reproduction by a third party with reasonable effort.
  We summarise and adapt previous recommendations for improving this dire picture and invite the GIScience community to start a broad discussion on reusability, quality, and openness of its research.
  The code and data for this article are published at https://doi.org/10.5281/zenodo.4032875.
bibliography: bibliography
authorrunning: "TODO authorrunning when order fixed" # mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'
# A "thin space" character, ' ' or &thinsp;, is used between the two first names.
copyright: "TODO copyright when order fixed, John Q. Public and Joan R. Public" # mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/
ccdesc:
  # Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
  concept_significance: "500"
  concept_desc: "Information systems~Geographic information systems"
keywords: "reproducible research, open science, reproducibility, GIScience"
# OPTIONAL:
acknowledgements: >
  Contributions (see [CRediT](https://casrai.org/credit/)) by BH: conceptualisation, investigation (21), writing – original draft, writing - review & editing; by DN: conceptualisation, investigation (33), software, writing – original draft, writing - review & editing, visualisation; by FO: conceptualisation, investigation (33), writing – original draft, writing - review & editing, software; by CG: conceptualisation, investigation (30), writing – original draft, writing - review & editing, software; by MK: conceptualisation, investigation (30), writing – original draft, writing - review & editing. The number of papers assessed by each coauthor are given in brackets after the "investigation" category.
#category: "Invited paper"
#relatedversion: "A full version of the paper is available at https://..." # optional, e.g. full version hosted on arXiv, HAL, or other respository/website
# optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
supplement: |
  The raw data for this work are the full texts of conference GIScience conference proceedings from the years 2012 to 2018 \cite{giscienceproceedings2012,giscienceproceedings2014,giscienceproceedings2016,giscienceproceedings2018}.
  The paper assessment results and source code of figures are published at [https://github.com/nuest/reproducible-research-at-giscience](github.com/nuest/reproducible-research-at-giscience) and archived on Zenodo \cite{daniel_nust_2020_4032875}.
  The used computing environment is [containerised with Docker and Binder-ready using R&nbsp;3.6.0 and Python&nbsp;3.5.3](https://github.com/rocker-org/binder/). R packages are installed from the [MRAN snapshot](https://mran.microsoft.com/timemachine) of July 5th 2019, and Python packages are pinned to specific versions, see `requirements.txt` file.
nolinenumbers: true # disable line numbering
hideLIPIcs: true # remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository
# appendix _after_ the bibliography
#appendix: |
output:
  bookdown::pdf_book:
    base_format: rticles::lipics_article
    keep_tex: TRUE
---


```{r load_data, echo=FALSE, message=FALSE, warning=FALSE}
library("here")

assessment_file <- here::here("results/paper_assessment.csv")

category_levels <- c("0", "1", "2", "3")
categoryColumns <- c("input data", 
                     "preprocessing",
                     "method/analysis/processing",
                     "computational environment",
                     "results")

paper_evaluation <- readr::read_csv(assessment_file, 
    col_types = readr::cols(
      `conceptual paper` = readr::col_logical(),
      `computational environment` = readr::col_factor(levels = category_levels),
      `input data` = readr::col_factor(levels = category_levels),
      `method/analysis/processing` = readr::col_factor(levels = category_levels),
      preprocessing = readr::col_factor(levels = category_levels),
      results = readr::col_factor(levels = category_levels)
      ),
    na = "NA")
```

---
appendix: |
  # Appendix
  
  ```{r corpus-table, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
  library("knitr")
  library("kableExtra")
  library("dplyr")
  
  knitr::kable(paper_evaluation %>%
                 select(#paper,
                        year, title, `conceptual paper`,
                        #`reviewer 1`,
                        #`reviewer 2`,
                        `input data`, `preprocessing`, `method/analysis/processing`, `computational environment`, `results`), 
        longtable = TRUE,
        booktabs = TRUE,
        #col.names = c("input data", "preproc.", "method/analysis/proc.",
        #              "comp. env.", "results"),
        caption = "Assessment data excerpt; for all fields (including assessors, authors, assessment comments) see reproducibility package at https://doi.org/10.5281/zenodo.4032875.") %>%
    kableExtra::row_spec(0, bold = TRUE) %>%
    kableExtra::column_spec(2, width = "20em") %>%
    kableExtra::column_spec(3, width = "6em") %>%
    kableExtra::column_spec(4, width = "3em") %>%
    kableExtra::column_spec(5, width = "3em") %>%
    kableExtra::column_spec(6, width = "3em") %>%
    kableExtra::column_spec(7, width = "3em") %>%
    kableExtra::column_spec(8, width = "3em") %>%
    kableExtra::kable_styling(latex_options = c("striped", "repeat_header", "scale_down"),
                              font_size = 6)
  ```
---

# Introduction

A large proportion of GIScience research today uses software to analyse data on computers.
This makes a considerable share of the articles published in the context of the GIScience conference series[^giscience] fall into the categories of data science or computational research.
Thereby these articles are exposed to challenges of transparency and reproducibility in the sense of the Claerbout/Donoho/Peng terminology \cite{barba_terminologies_2018}, where reproduction means a recreation of the same results using the same input data and workflow as the original authors.
In previous work \cite{nust_reproducible_2018} we assessed the reproducibility of a selection of full and short papers of the AGILE conference series[^agile], a community conference organised by member labs of the Association of Geographic Information Laboratories in Europe (AGILE).
The AGILE conference is closely related to GIScience conference in terms of scientific domain and contributing authors.
Using systematic analysis based on a rubric for reproducible research, we found that the majority of AGILE papers did neither provide sufficient information for a reviewer to evaluate code and data and attempt a reproduction, nor enough material for readers to reuse or extend data or code from the analytical workflows.
This is corroborated by research in related disciplines such as quantitative geography \cite{brunsdon_quantitative_2016}, qualitative GIS \cite{muenchow_reviewing_2019}, geoscience \cite{konkol_computational_2018}, and e-Science \cite{freire_et_al:DR:2016:5817}.
The problems identified in these related research areas are directly transferable to GIScience, which operates at the intersections of aforementioned fields \cite{Goodchild1992}.
In any case, observations on the lack of reproducibility in all scientific fields contrast with the clear advantages and benefits of open and reproducible research both for individuals and for academia as a whole (cf. for example \cite{donoho_invitation_2010,markowetz_five_2015,kray_reproducible_2019,Colavizza2020}).
As a consequence, we have initiated a process to support authors in increasing reproducibility for AGILE publications, which produced as main output author guidelines as well as strategies for the AGILE conference series[^reproagile].

An obvious question is whether the GIScience conference series faces the same issues, and whether similar strategies could work for improvement.
To start with this investigation, we conducted a simple text analysis of GIScience proceedings[^textanalysis] to evaluate the relevance of computational methods in the conference papers.
The analysis searches for several wordstems related to reproducibility: generic words, e.g., "data", "software", or "process"; specific platforms, e.g., "GitHub"; and concrete terms, e.g., words starting with "reproduc" or "replic".
Table&nbsp;\@ref(tab:wordstem-table) the results of the search per year.
The take-away message from the text analysis is that algorithms, processing, and data play a large role in GIScience publications, but only a small mention of code repositories or reproduction materials could be identified.
Therefore, a more detailed manual assessment of the reproducibility of these publications seems necessary.

The main contribution of this work addresses two objectives:
First, investigate the state of reproducibility in the GIScience conference community.
This investigation broadens our knowledge base about reproducibility in GIScience in general and teaches us more about the situation in the GIScience conference series specifically.
Second, apply the assessment procedure used for AGILE conference papers (presented in the section on the Reproducibility Assessment) to the papers of the GIScience conference, so that its broader suitability is evaluated using a different dataset.
Such a transfer validates the developed methodology.
Together, these objectives yield important findings for the discussion of reproducibility within the GIScience conference community and the GIScience discipline at large. 
Only then can a fruitful dialogue take place on if and how to improve reproducibility for the GIScience conference series, and whether the recent steps taken at AGILE[^reproagile] could be an inspiration for GIScience conferences as well.
We discuss these findings and present our conclusions in the final two sections (Discussion; Conclusions and Outlook).

[^giscience]: [https://www.giscience.org/](https://www.giscience.org/)
[^agile]: [https://agile-online.org/conference](https://agile-online.org/conference)
[^textanalysis]: The full text analysis and the results is available in this paper's repository in the following files: [`giscience-historic-text-analysis.Rmd`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-historic-text-analysis.Rmd) contains the analysis code; the result data are two tables with counts for occurences of words respectively word stems per year in [`results/text_analysis_topwordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_topwordstems.csv) and [`results/text_analysis_keywordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_keywordstems.csv); a wordcloud per year is in file [`results/text_analysis_wordstemclouds.png`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_wordstemclouds.png).
[^reproagile]: See the initiative website at [https://reproducible-agile.github.io/](https://reproducible-agile.github.io/), the author guidelines at [https://doi.org/10.17605/OSF.IO/CB7Z8](https://doi.org/10.17605/OSF.IO/CB7Z8) and the main OSF project with all materials [https://osf.io/phmce/](https://osf.io/phmce/) \cite{reproducible_agile}.

```{r wordstem-table, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
keywordstems_counts <- read.csv(here::here("results/text_analysis_keywordstems.csv"))

library("knitr")
library("kableExtra")

# for inline testing: kable(word_counts_sums)
knitr::kable(keywordstems_counts,
             col.names = c("year", "words",
                           "reproduc..",
                           "replic..",
                           "repeatab..",
                           "code",
                           "software",
                           "algorithm(s)",
                           "(pre)process..",
                           "data.*",
                           "result(s)",
                           "repository/ies",
                           "github/lab"),
             booktabs = TRUE,
             caption = "Reproducibility-related word stems in the corpus per year of proceedings") %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling(latex_options = c("striped", "scale_down"),
                            font_size = 6) %>%
  kableExtra::footnote(general = "The very high value for 'code' in 2012 is due to a single paper about land use, for which different codes are defined, discussed and used.",
                       title_format = c("italic"))
```

# Related work

This work transfers a previously used method to a new dataset and the original article, \cite{nust_reproducible_2018}, already provides an overview of reproducible research in general, including definitions, challenges, and shortcomings. In the following, we focus on works recently published and briefly introduce related meta-studies.

Few groups have attempted practical reproduction of computational works related to GIScience.
\cite{konkol_computational_2019} conducted an in-depth examination of the computational reproducibility of 41 geoscience papers with a focus on differences between the recreated figures. The set of papers was, similar to our work, drawn from a fixed group of two outlets (journals), but was further limited to recent papers providing code in the R language.
Konkol et al. (2019) also conducted actual reproductions, from which we abstained.
However, applying the same prerequisites to our dataset would be possible as a next step.
One could attempt to reproduce the papers whose assessment points to a possible reproduction (i.e., level two or higher, see below).
In a report on the reproducibility review at the AGILE conference 2020[^reproreport], the reproducibility committee summarises the process and documents relevant obstacles to reproducibility of accepted papers.
The main issues identified in the reproducibility committee's report and by \cite{konkol_computational_2019} are quite coincident.

Within the geospatial domain, \cite{kedron_reproducibility_2020} provide a recent review of opportunities and challenges for reproducibility and replicability.
They transfer solutions from other domains but also discuss and conceptualize the specific nature of a reproducibility and replicability framework when working with geospatial data, e.g., handling context, uncertainty of spatial processes, or how to accomodate the inherent natural variability of geospatial systems.
In a similar manner, \cite{brunsdon_opening_2020} investigate reproducibility within spatial data science, with particularly discussing big spatial data.
They support the need for open tools, knowledge about code, and reproducibility editors at domain journals and conferences, but also introduce the perspective that spatial analysis is no longer conducted only by GI/geo-scientists or geographers and connect reproducibility with critical spatial understanding.
The conceptual work in these articles complements the assessment of reproducibility conducted in this paper.

Two recent studies from distant disciplines, wildlife science \cite{archmiller_computational_2020} and hydrology \cite{stagge_assessing_2019}, relate to our work in this paper.
Both investigate a random set of articles from selected journals and use step-wise process of questions to determine availability of materials, and eventually reproduce workflows if possible.
\cite{archmiller_computational_2020} use a final ranking of 1 to 5 to specify the degree to which a study's conclusions were eventually reproduced.
Similar to our classification scheme, their ranking models the general notion of a _"reproducibility spectrum"_ \cite{peng_reproducible_2011}.

[^reproreport]: [https://osf.io/7rjpe/](https://osf.io/7rjpe/)

# Reproducibility assessment method

## Criteria

The assessment criteria were originally defined in previous work \cite{nust_reproducible_2018}.
We provide a short introduction here and refer to \cite{nust_reproducible_2018} for details.
The three assessment criteria are _Input Data_, _Methods_, and _Results_.
_Input Data_ comprises all datasets that the computational analysis uses.
_Methods_ encompasses the entire computational analysis that generates the results.
Since _Methods_ is difficult to evaluate as a whole, we split this criterion into three subcategories:
_Preprocessing_ includes the steps to prepare the _Input Data_ before the main analysis;
_Methods, Analysis, Processing_ is the actual analysis; 
_Computational Environment_ addresses the description of hard- and software.
Finally, the criterion _Results_ refers to the output of analysis, for example, figures, tables, and numbers. 

For each of these (sub)categories, we assigned one of four levels unless the criterion was not applicable, _(Level NA)_:
_(Level 0) Unavailable_ means that it is not possible to access the paper’s data, methods, or results, and that it is impossible to recreate them based on the description in the paper.
_(Level 1) Documented_ indicates that the paper still does not provide direct access to datasets, methods, or results, but that there is sufficient description respectively metadata to recreate them closely enough for an evaluation, yet often a recreation is unlikely due to huge efforts needed.
With regard to the methods criteria, Level 1 means that pseudo code or a textual workflow description is available.
_(Level 2) Available_ is assigned if the paper provides direct access to the materials (e.g., through a personal or institutional website), but not in the form of an open and permanent identifier, such as a DOI.
The indication of a DOI does not apply to the methods criteria as the permanent reference to code, libraries and system environments with a single identifier is not yet common practice.
The gold standard, _(Level 3) Available and open_, requires open and permanent access to the materials (e.g., through public online repositories) and open licenses to allow use and extension.

Note that levels are ordinal numbers that can be compared (3 is higher than 2), but absolute differences between numbers must not be interpreted as equals.
Moving one level up from 0 to 1 is not the same as from 1 to 2.
While reaching 1 is fairly straightforward, jumping to level 2 means to almost have a fully reproducible paper.

## Process

The overall approach to assessing the reproducibility of GIScience papers, again, roughly follows the previous assessment of AGILE papers \cite{nust_reproducible_2018}.
All full papers in the GIScience conference series (from the 2012 to 2018 editions) were assessed.
This is partly because of the absence of a provided subset, such as the nominees for best papers as in the case of the AGILE conference series, and partly to work with a larger dataset for potentially more informative results.
Each GIScience paper was randomly assigned to two assessors who evaluated it qualitatively according to the five reproducibility criteria.
The assessors were free in the way they approached the assigned works; the approach depends on the structure of the paper and the familiarity with the topic, and could range from a quick browse to identify relevant statements to a thorough reading of the full text.
The identification of relevant content could be supported to some extent by a PDF reader with multiple highlights, using keywords like e.g. "data, software, code, download, contribution, script, workflow".
The results of the individual assessments were joined in a collaborative Google Spreadsheet.
This spreadsheet also has a comments column to record relevant sources and decisions.
When there was disagreement between assessors, arguments for and against a certain reproducibility level were discussed in the entire group of five assessors until a consensus was reached.
Only then the assessments were merged into a single value, one year at a time.
A snapshot of both the unmerged and merged values was stored as a CSV file in the collaboration repository for transparency and provenance[^assessmenttable].
Two independent assessors per paper increased the objectivity of the final assessment.
Disagreements were found helpful in aligning the interpretation of criteria and, in rare cases, lead to an adjustment of similar cases in other papers.

The discussion about the correct assignment of levels made us reflect in the application of the rubric for some special situations. For the input data criterion, some papers had input data “available” at the time of writing/publication but it was not the case at the present time, due to causes such as broken links, changes in the URL structure of a website, and projects and/or personal websites that are down or moved. In such a cases, we gave the authors the benefit of the doubt and assumed the data was accessible some time after the publication of the conference proceedings. We did not give it an arbitrary score and discussed internally the best level per case; yet, these papers never earned a `3` (which includes permanency). Related to this criterion, simulation data like the specification or configuration of agents in an agent-based system was not treated as input data (data NA), but as parameters of the main analysis (being part of the methods/analysis/processing criterion). 

Preprocessing covers preparatory work for the actual analysis involving as varied tasks as data selection, cleaning, aggregation and integration. However, the dividing line between data preprocessing and processing (i.e. main analysis) is often thin and not always well-defined. Assessors' opinions were not in agreement several times regarding the applicability of the preprocessing criterion in terms of 'not applicable' or 'applicable'; if it was applicable, there was unanimity on whether it was not reproducible at all or hardly reproducible (`0` or `1`, respectively). Therefore, it was decided to apply the rubric in cases where papers specifically mentioned preprocessing task independently from the actual analysis or method, which was sometimes clearly stated in a paper by separate sub-sections.

Lastly, human subject tests and surveys were also a special case. Human-related research activities were rated as `1` in the methods/analysis/processing criterion if sufficiently documented; although a sufficient documentation in these cases did not mean that original sources were available or could be exactly recreated. 

[^assessmenttable]: The assessment results are in the file [`results/paper_assessment.csv`](). As an example, commit [`464e630`](https://github.com/nuest/reproducible-research-at-giscience/commit/464e63003a550db216c827571e503d464615efc7) and  [`2e8b1be`](https://github.com/nuest/reproducible-research-at-giscience/commit/2e8b1bebd6298c5ccfd61af1f67a75c082136d3e) are the pre-merge and post-merge commit after completing the assessment of the papers from 2014. The pre-merge commit contains the assessments including the assessors' initials, e.g. "CG: 1, MK: 1".

## Paper corpus

```{r load_data, echo=FALSE, message=FALSE, warning=FALSE}
library("here")

assessment_file <- here::here("results/paper_assessment.csv")

category_levels <- c("0", "1", "2", "3")
categoryColumns <- c("input data", 
                     "preprocessing",
                     "method/analysis/processing",
                     "computational environment",
                     "results")

paper_evaluation <- readr::read_csv(assessment_file, 
    col_types = readr::cols(
      `conceptual paper` = readr::col_logical(),
      `computational environment` = readr::col_factor(levels = category_levels),
      `input data` = readr::col_factor(levels = category_levels),
      `method/analysis/processing` = readr::col_factor(levels = category_levels),
      preprocessing = readr::col_factor(levels = category_levels),
      results = readr::col_factor(levels = category_levels)
      ),
    na = "NA")
```

```{r count_papers, echo=FALSE, message=FALSE, warning=FALSE}
library("dplyr")

count_papers <- nrow(paper_evaluation)

count_conceptual <- nrow(paper_evaluation %>% 
                           dplyr::filter(`conceptual paper` == TRUE))
                           
count_mixed <- nrow(paper_evaluation %>% 
                      dplyr::filter(is.na(`input data`) 
                             | is.na(preprocessing) 
                             | is.na(`method/analysis/processing`) 
                             | is.na(`computational environment`) 
                             | is.na(results)))

paper_evaluation_wo_conceptual <- filter(paper_evaluation, `conceptual paper` == FALSE)

count_n <- nrow(paper_evaluation_wo_conceptual)
```

```{r conceptual_papers, echo=FALSE, message=FALSE, warning=FALSE}
library("tidyr")

conceptual_paper_counts <- paper_evaluation %>%
  dplyr::filter(`conceptual paper` == TRUE) %>%
  group_by(year) %>%
  summarise(n = n()) %>%
  unite(counts, year, n, sep = ": ")
```

In total, `r count_papers` papers from the GIScience conferences in `r paste(sort(unique(paper_evaluation$year)), collapse = ", ")` were assessed.
Table&nbsp;\@ref(tab:corpus-table) shows the full results of the assessment, for details on assigned assessors, authors, etc. please check the reproducibility package.
`r count_conceptual` papers across all years were identified as conceptual papers[^conceptual] and were not included in the corpus.
This results in `r paste0(round(count_conceptual / count_papers * 100, 0), "%")` out the corpus.
For comparison, our previous reproducibility assessment of the AGILE conference series' papers had 5 conceptual papers out of 32 (`r paste0(round(5 / 32 * 100, 0), "%")`) - see \cite{nust_reproducible_2018} for further details -, which shows a similar proportion, even though both corpora have different time windows.
The number of conceptual papers in GIScience conferences was low over the analysed years (`r paste(conceptual_paper_counts$counts, collapse = "; ")`, going down to 0 in the last year's proceedings (`r max(unique(paper_evaluation$year))`).
This might suggest the increasingly predominant and ubiquitous role of analytics datasets and computational workflows in the generation of the final published results in the field.
In summary, `r count_n` were assessed according to the reproducibility criteria as explained next.

# Reproducibility of GIScience conference papers

Table&nbsp;\@ref(tab:summary-evaldata) shows aggregated values for the assessed reproducibility levels. 
    
`r paper_evaluation_wo_conceptual %>% filter(is.na(preprocessing)) %>% count() %>% .$n` papers are not applicable for the preprocessing criterion.
This can be explained because the boundary between the preprocessing criterion and the methods/analysis/processing criterion is not always clear in a given paper and is therefore subject to the reader's interpretation.
This does not mean that the papers that did not qualify for the preprocessing criteron are not reproducible at all.
Simply, in these cases, either no preprocessing is required or has been integrated into the main analysis.
Obviously, the ability to reproduce a paper is seriously limited if data preprocessing is required but it is either not indicated in the paper or is not provided as an additional (computational) resource.

If we look at the median values of the five criteria, a typical GIScience paper scores `11101`.
This rubric translates in practical terms into a paper that's sufficiently documented to claim that reproduction could be attempted within a short time frame after publication.
While such a level of reproducibility is typically accepted by journals and conferences today, it does not guaranttee actual reproducibility or even reproduction.
If we had to reproduce such a typical paper, we would require a lot of effort, technical skills, communication with authors, and time to both gather, recreate, and/or analyse all the necessary resources (data, code, etc.), and to recreate the specific computational environment of the paper, if the latter were possible after all since it is not specified on average (note the value `0` in the forth position in `11101`).

```{r criteria_numbers, echo=FALSE, message=FALSE, warning=FALSE}

data_level_zero <- paper_evaluation_wo_conceptual %>% 
  filter(`input data` == 0) %>% 
  count() %>% .$n

data_level_two <- paper_evaluation_wo_conceptual %>% 
  filter(`input data` == 2) %>% 
  count() %>% .$n

preprocessing_included <- paper_evaluation_wo_conceptual %>% 
  filter(!is.na(preprocessing)) %>% 
  count() %>% .$n

preprocessing_level_one <- paper_evaluation_wo_conceptual %>% 
  filter(preprocessing == 1) %>% 
  count() %>% .$n

methods_and_results_eq_one <- paper_evaluation_wo_conceptual %>% 
  filter(`method/analysis/processing` == 1 & results == 1) %>% 
  count() %>% .$n
  
compenv_level_zero <- paper_evaluation_wo_conceptual %>% 
  filter(`computational environment` == 0) %>% 
  count() %>% .$n
  
```

Figure&nbsp;\@ref(fig:assessment-results) shows the distribution of the reproducibility levels for each criterion.
None of the papers reaches the highest level of reproducibility in any criterion.
Only `r data_level_two` papers reach level 2 in the input data criterion, which is still the highest number of that level across all criteria.
Similar to previous results \cite{nust_reproducible_2018}, the number of papers with level 0 for data is especially high (`r data_level_zero`, `r paste0(round(data_level_zero / count_n * 100, 0), "%")`), which is a heavy barrier to reproduction since input data is not only unavailable, but cannot be recreated from the information provided in the paper.

Data preprocessing applies to `r preprocessing_included` publications, and the levels reached are generally low (0,1), being level 2 almost residual. `r preprocessing_level_one` papers reach level 1 in the  data preprocessing criterion.
This roughly represents half of the papers with level 1 in the analysis criterion, suggesting that sometimes data preproceesing was clearly identified in some papers while in others it was unclear to decide either the existence of data preprocessing tasks separated from the main analysis or being part of it.
The former case, data processsing steps required but not reported, is much more problematic as it notably hinders the reproduction of the results of a paper.

Methods and results criteria show a pretty similar distribution (Figure&nbsp;\@ref(fig:assessment-results).
Indeed, `r methods_and_results_eq_one` publications have level 1 in both criteria, which represents `r paste0(round(methods_and_results_eq_one / count_n * 100, 0), "%")` of the papers assessed.
In this sense, most of the assessed papers fall below the minimum standard for reproduction as regards the methods and results criteria. 
All papers except one have "1" for the results criterion, which shows that the peer review worked as expected for almost all articles.
In other words, the authors are concerned with making the results understandable to the reviewers, which is not always the case for the rest of the criteria.
More generally, this aspect raises the question of whether peer review should stop in the absence of minimal evidence of the input data, analysis, and computational environment used in a paper.
Without an explicit justification (confidential data, privacy concerns, etc), should a paper with level 0 (or NA) for the input data criterion get accepted for a conference? 

Finally, the computational environment criterion obtains the worst results. `r compenv_level_zero` (`r paste0(round(compenv_level_zero / count_n * 100, 0), "%")`) publications are level 0, which means that no clues are provided in the paper about the computing environment, tools and libraries used in the reported research and/or analysis.
The computational environment criterion and the input data criterion account for a considerable number of `0s`, which clearly signals a serious impediment to reproduction.


The full assessment table in the paper repository is at [`paper_assessment.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/paper_assessment.csv) 

```{r summary-evaldata, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
library("knitr")
library("kableExtra")

evaldata_numeric <- paper_evaluation_wo_conceptual %>%
  # must convert factors to numbers to calculate the mean and median
  dplyr::mutate_if(is.factor, dplyr::funs(as.integer(as.character(.))))

summaryna <- function (v) {
  if(!any(is.na(v))){
    res <- c(summary(v),"NA's"=0)
  } else{
    res <- summary(v)
  }
  return(res)
} 


# apply summary independently to format as table
summaries <- sapply(evaldata_numeric[,categoryColumns], summaryna)
exclude_values_summary <- c("1st Qu.", "3rd Qu.")
kable(subset(summaries, !(rownames(summaries) %in% exclude_values_summary)), 
      digits = 1,
      col.names = c("input data", "preproc.", "method/analysis/proc.",
                    "comp. env.", "results"),
      caption = "Statistics of reproducibility levels per criterion (rounded to one decimal place)")
```

```{r assessment-results, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%', fig.cap="Barplots of reproducibility assessment results; levels range from 0 (leftmost bar) to 'not applicable' (rightmost bar)."}
# match the colours to time series plot below
colours <- RColorBrewer::brewer.pal(length(categoryColumns), "Set1")
level_names <- c("0", "1", "2", "3", "NA")

criteriaBarplot = function(data, main, colour) {
  barplot(table(data, useNA = "always"), 
          main = main,
          xlab = "Level",
          ylim = c(0,nrow(paper_evaluation_wo_conceptual)),
          names.arg = level_names, col = colours[colour])
}

par(mfrow = c(1,length(categoryColumns)),
    cex = 0.5,
    cex.axis = 0.9)
criteriaBarplot(paper_evaluation_wo_conceptual$`input data`,
                main = "Input data", colour = 1)
criteriaBarplot(paper_evaluation_wo_conceptual$`preprocessing`, 
                main = "Preprocessing", colour = 2)
criteriaBarplot(paper_evaluation_wo_conceptual$`method/analysis/processing`,
                main = "Methods/Analysis/\nProcessing", colour = 3)
criteriaBarplot(paper_evaluation_wo_conceptual$`computational environment`,
                main = "Computational\nEnvironment", colour = 4)
criteriaBarplot(paper_evaluation_wo_conceptual$results,
                main = "Results", colour = 5)
```

[^conceptual]: See \cite{nust_reproducible_2018} for a definition of "conceptual".

# Discussion

## State of reproducibility in the GIScience conference series

Our first research objective was to assess the state of reproducibility in the GIScience conference series.
A recurrent issue found in the analysis is the inability to access input data based on the information provided in the paper.
Most of the links and pointers to datasets reported at the time of publication were either broken (e.g., non-existing resource, HTTP 404 error, invalid URL syntax) or not available anymore (URL works but is redirected to a different generic page; specific resource from the paper no longer exists).
In these cases, a level of 2 in the input data criterion was well deserved at the time of publication.
However, when evaluating the level of reproducibility some time later, as done in this work, level 2 is no longer correct for those papers.
From the reproducibility point of view input data is not accessible, even if authors could be contacted directly.
In the meaning of the criterion and in practical terms, this is effectively the same as the statement "available upon request" and thereby level 0.
An important part of reproducibility is that access to material should not degrade over time, which is best achieved by depositing data in repositories, including mechanisms for sensitive data, and properly citing it.
In this assessment of reproducibility, we decided to give the authors the benefit of the doubt and awarded a value of 2 even if we could not conclusively determine, e.g., by using the Internet Archive's Wayback Machine[^wayback], if the original website ever existed.
<!-- DN: to me, this goes into too much detail about good practice for data citation.

Simple methods and techniques can be more easily adhere to this goal, even for decades[^tenopir].
Instead of short-lived URLs, e.g., a personal or project website, authors should archive input data used in a paper in permanent repositories.
Consequently, footnotes with URLs are not suitable for citing data, but as a citation by itself like any other resource or paper included in the bibliography section.
-->

[^wayback]: [https://web.archive.org/](https://web.archive.org/).

Regarding the common situation of a paper with "all 1s", our interpretation is that indeed this is a regular paper that is up to current scientific standards.
Even if there is a 0 in some criteria, that does not mean that the paper should not have passed peer review - after all, we assess specifically a paper's reproducibility, which likely was not included at all in the call for papers or the reviewer guidelines, and has therefore not received much attention from authors or reviewers.
We analyse the history, when there were no incentives to push these aspects.

However, the overall picture matches a somewhat worrisome focus on publishing exciting results in science, and less care about publishing the full process that allows readers to try and fully understand the research. 
Clearly, a score of 0 in input data is problematic, because without sufficient knowledge about the characteristics of the input data, all attempts at reproducing results are bound to fail, even when the textual documentation would potentially allow an expensive recreation of the method.

## Transferability of method

Concerning our second research objective, we can state that the process and the rubric worked, but faced similar challenges as we recalled from its first application.
The preprocessing criterion caused many discussions among the reproducibility reviewers during the assessment.
It is often not clear or a matter of interpretation if a particular processing step belongs to a minor basic transformation of input data, if it is already part of the main analysis, and when it is a truly distinct step in the process.
The borders are vague and hence scores should be interpreted with caution.
Future assessments could provide a more precise definition for _pre_-processing, e.g., only use it if the authors use the term, or might consider to drop the category.

In a similar vein, the computational environment is difficult to clearly distinguish from analysis, and technology and practices for the effective management of the computing environment are reached mature states relatively recently.
Future assessments could prepare concrete rules as reliefs for historic workflows, similar as discussed for input data above.

Furthermore, it is important to remember that the levels of reproducibility are not equidistant in the sense that a level of 2 would be twice as good as a level of 1, or that the effort needed is twice as high.
A level of 1 should be the standard for peer-reviewed papers.
Moving it to 2 requires several steps and actions, while reaching the gold standard of 3 is a comparetively small step from level 2 - the main difference is to use public repositories with a DOI - but with a comparatetively positive impact in permanent accessibility.

## Comparison of conferences

Given that we followed the same process as in \cite{nust_reproducible_2018}, and the demonstrated transferability of the method, a comparison between the two conference series seems appropiate.
It is important to remember that we do not attempt such a comparison with the objective to declare a 'winner'.
The two conferences are similar enough, as shown below, for a comparison, yet too distinct and diverse in set-up, process, and audience for such simplistic ranking.
However, a comparison is also required for a sensible discussion of whether the guidelines developed for AGILE might be promising for adaptation at GIScience - are they transferable, and if not so, what adaptations seem necessary?

```{r author_analysis, echo=FALSE, message=FALSE, warning=FALSE}
author_counts <- read.csv(here::here("author_analysis", "author_counts.csv"))
```

Concerning the contributing and participating academic communities, \cite{egenhofer_contributions_2016,kemp2013results} include both conferences considered here as outlets for GIScience.
\cite{kesler_spatiallinkedscience_2012} study the bibliographies of four GIScience conference series, including GIScience and AGILE.
They list 15 authors who have published in all GIScience conference.
We conducted a cursory investigation of the body of authors for full papers revealing significant overlap[^authoranalysis]: Out of `r author_counts$AGILE` unique AGILE and `r author_counts$GIScience` unique GIScience full paper authors, `r author_counts$intersection` published in both conferences, who include all 15 authors mentioned in \cite{kesler_spatiallinkedscience_2012}. 
Therefore, the strong relation between the AGILE and GIScience conference series and provide the foundation for applying the same methodology to GIScience that has been developed for AGILE conference publications, and probably result in similar implications for improving reproducibility.

Concerning the paper corpora, the publication years considered here are similar to the assessment of AGILE papers, which makes results comparable in the sense of what methods and tools would have been available for authors.

Furthermore, we note that both conferences have a similar ratio of conceptual papers which were not assessed for reproduciblity: .... TODO DANIEL: sentence with the numbers for conceptual papers.
This indicates that both conferences have similar share of papers using computational methods.
On the content of the papers, our overall impression is that a larger share of GIScience papers focus on theoretical, conceptual, or methodological aspects than at AGILE, while AGILE papers seem to feature more empirical and/or applied geoinformation research.


|  Mean values per criterion| AGILE    | GIScience |
| -------- | -------- | -------- |
| Input data      | 0.48     | 0.7     |
| Computational environment     | 0.46     | 0.3     |
| Results     | 0.78    | 1.1     |

Regarding the results of the reproducibility assessment, the nature of the data and sample size does not suggest statistical analysis about significant differences.
However, looking at the _input data_ category, GIScience has a slightly higher mean (0.7 as opposed to 0.48 for AGILE) and a median of 1, indicating a slightly better, but by no means optimal, availability of input data.
The pattern of reproducibility of the papers' analyses is very similar for the two conference series.
The overwhelming majority of papers achieve a level of 1, resulting in a mean of 1.0 for GIScience and 0.98 for AGILE.
The _computational environment_ category is not that different, but sees AGILE in front with a mean of 0.46 vs. 0.3 for GIScience.
The _results_ category scores are again slightly higher for GIScience, with a mean of 1.1 vs. a mean of 0.78 for AGILE. There are several papers in AGILE that received a score of 0 here, indicating that crucial information is missing about elements and transformations between analysis outputs and presented results.
We refrain from comparing the _preprocessing_ category here, because our analysis has shown that this is a somewhat contentious dimension, causing the majority of discussions between assessors.

An important difference between the two conference series is that GIScience is a biannual conference series as opposed to the annual AGILE, and features a different pre-publication review process and review management system: in AGILE both authors and reviewers are anonymous, while in GIScience only the reviewers are.

The AGILE conference series has the AGILE association[^agileorg] as institutional supporter, which means a more stable organizational and financial framework for activities spanning more than one or between conferences.
However, like GIScience, the local conference organizers have to lift the main financial burden.
AGILE also has a different geographic focus: While GIScience has a global target audience, the individual conferences are likely to be different in their contributor communities, because of the moving conference location, which often means lowered accessibility for other parts of the world.
AGILE, by comparison, has a European focus, and although the conference location moves every year, accessibility is less diverse. 
This likely translates into a less fluctuating, and less geographically diverse audience.

This comparison lets us draw two main conclusions:
First, that both the target audience and the content of the two conference series are similar enough to be afflicted with similar shortcomings in terms of reproducibility, and thus likely also similar solutions.
Second, that the AGILE conference series seems structurally better positioned to support changing habits, because of a more stable audience and institutional support.
The introduction of the AGILE reproducibility guidelines was done within a short time frame and with financial support in form of an "AGILE initiative" for an in-person workshop.
For GIScience, the task to change the review process to support more reproducibility falls squarely on the shoulders of the changing program committees. 
However, the experience of AGILE's new guidelines shows that even small changes can lead to a significantly improved outcome.

[^tenopir]: See Tenopir and colleagues' trilogy about "data sharing" at [https://journals.plos.org/plosone/search?filterJournals=PLoSONE&filterAuthors=Carol+Tenopir&page=1](https://journals.plos.org/plosone/search?filterJournals=PLoSONE&filterAuthors=Carol+Tenopir&page=1).
[^agileorg]: [https://agile-online.org/](https://agile-online.org/).
[^authoranalysis]: The data and code for the brief exploration into the authorships across the conferences considered in this work can be found in the directory `author_analysis` of this paper's reproducibility package \cite{daniel_nust_2020_4032875}.

# Conclusions and outlook

In this work we investigated the reproducibility of several years of GIScience conference publications.
The paper corpus is large enough for a representative sample.
The corpus size is comparable to the one used for the AGILE study, but the corporas have different time windows.
It was never the intention of this study to rate the papers, or engage in a comparison of AGILE vs. GIScience conference quality.
We also do not question that the research presented in these papers is sound and relevant, since they were accepted for publication at a reputable conference.
Instead, we investigated the papers along a single desirable quality dimension, reproducibility, which implies requirements on openness and transparency.

Taking a similar high bar for reproducibility as in the earlier study, the results clearly show a lot of room for improvement, as none of the presented articles was readily reproducible.
The majority of articles provide some information, but not to the degree required to facilitate transparent, and, most relevantly, reusable research using data and software.
This is overall similar to the outcomes of our earlier study on AGILE papers.
In \cite{nust_reproducible_2018} we describe concrete recommendations for individuals and organisations to improve the reproducibility.
We have argued that AGILE and GIScience share a sufficiently common starting position, domain/discipline characteristics, attendence, audience, and author community, for sharing common strategies to improve the situation.
Therefore the recommendations are directly transferable to the GIScience conference series, most importantly (1) a promotion of outstanding reproducible work, e.g., with awards or badges,
(2) recognition of the efforts to achieve reproducibility, e.g., with a special track for reproducible papers, a role of reproducibility reviewer, open educational resources, and helpful author guidelines including data and software citation requirements and a specific data/software repository, and
(3) an institutional commitment to a policy shift that goes beyond mere accessibility \cite{stodden_empirical_2018}.
These changes require a roadmap and clear year, say 2024, when GIScience starts to only accept computationally reproducible submissions and reproducibility is checked before acceptance of papers. The concluding statement of \cite{archmiller_computational_2020} is directly transferable to GIScience: the challenges are not insurmountable and increased reproducibility will ensure scientific integrity.

The AGILE reproducible paper guidelines \cite{agile_guidelines} and the associated reproducibility review processes as well as community code review systems such as CODECHECK \cite{eglen_codecheck_2019} are open and "ready to use".
They can also be adopted for the GIScience conferences, e.g., to suit the peer review process goals and scheduling.
\cite{kedron_reproducibility_2020} stress the need for a comprehensive balanced approach to technical, conceptual, and practical issues.
They further point out that availability must not lead to adoption.
Therefore, a broad discourse around these recommendations, tools, and concepts would be beneficial for all members of the community, whether their work is more towards the conceptual, the computational, or the applied direction of GIScience.
A survey for authors, as conducted for AGILE \cite{nust_reproducible_2018}, could help identify specific hindering circumstances for and special requirements of the GIScience conference community, beyond the findings presented here and in related work.

Future work may transfer the assessment process to other major events and outlets for GIScience research, such as GeoComputation or COSIT conferences, or domain journals (cf. \cite{egenhofer_contributions_2016} for an extensive list), although we would not expect significantly differing results.
Practical reproductions of papers and even replications of fundamental works are even more promising projects to convincingly underpin a call for a culture change.
For example, \cite{egenhofer_contributions_2016} provides for a list of most frequently cited aticles as candidates.
Such a project would ideally be supported with proper funding.
If the observed trend is continued in further, or in case of replications much more detailed evaluations, which we fear it would, the GIScience community should take action and improve its state of reproducibility.
A timely adoption of the technological and procedural solutions may allow GIScience researchers, together with all of academia, to level up and approach the challenges of the _"second phase of reproducible research"_ by tacklung long-term funding for maintenance of code and data and building supporting infrastructure for reproducible research \cite{peng_reproducible_2020}.

```{r wordcount, echo=FALSE, message=FALSE, warning=FALSE}
message("WORDCOUNT: ", wordcountaddin::word_count())
```

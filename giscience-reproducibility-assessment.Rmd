---
title: 'Reproducibility assessment data analysis for "Reproducible research and GIScience: an evaluation using GIScience conference papers"'
author: "Daniel Nüst, Carlos Granell"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
    code_download: true
    code_folding: hide
    self_contained: false
    lib_dir: libs
urlcolor: blue
---

```{css, echo=FALSE}
pre {
  font-size: 12px;
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
```

This document visualises the expert assessment for accepted full papers at the [GIScience conference series](https://www.giscience.org/) for the following conferences and proceedings.

- 2018: Tenth International Conference on Geographic Information Science, Melbourne, Australia
  
  _Proceedings 10th International Conference on Geographic Information Science (GIScience 2018). 2018. In Winter, S., Griffin, A., Sester, M. (Eds.), LIPICS Vol. 114. ISBN 978-3-95977-083-5. http://www.dagstuhl.de/dagpub/978-3-95977-083-5 _

- 2016: Ninth International Conference on Geographic Information Science, Sep 27, 2016 - Sep 30, 2016, 	Montreal, Canada
  
  _Geographic Information Science. 2016. In J. A. Miller, D. O’Sullivan, & N. Wiegand (Eds.), Lecture Notes in Computer Science. Springer International Publishing. https://doi.org/10.1007/978-3-319-45738-3 _

- 2014: Eighth International Conference on Geographic Information Science, Sep 24, 2014 - Sep 26, 2014, 	Vienna, Austria
  > Geographic Information Science. 2014. In M. Duckham, E. Pebesma, K. Stewart, & A. U. Frank (Eds.), Lecture Notes in Computer Science. Springer International Publishing. https://doi.org/10.1007/978-3-319-11593-1

- 2012: Seventh International Conference on Geographic Information Science, Sep 18, 2012 - Sep 21, 2012, Columbus, Ohio, USA
  > Geographic Information Science. 2012. In N. Xiao, M.-P. Kwan, M. F. Goodchild, & S. Shekhar (Eds.), Lecture Notes in Computer Science. Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-33024-7

The analysis is based on the assessment first published in _"Reproducible research and GIScience: an evaluation using AGILE conference papers"_ ([https://doi.org/10.7717/peerj.5072](https://doi.org/10.7717/peerj.5072)), see [https://github.com/nuest/reproducible-research-and-giscience](https://github.com/nuest/reproducible-research-and-giscience).

## Reproduce paper

To create the assessment result data and figures based on this document you can run the following commands in a new R session after completing the prerequisites with the original paper corpus data.
If you have problems rendering the document you can execute each chunk independently in [RStudio](https://rstudio.com/products/rstudio/).

```{r render_with_rmarkdown, eval=FALSE, class.source = "fold-show"}
# Following code is not executed when document is created
require("knitr")
require("rmarkdown")
rmarkdown::render("giscience-reproducibility-assessment.Rmd")
```

## Software dependencies and code

This document does not install the required R packages by default.
You can run the script `install.R` to install all required dependencies on a new R installation, or use `install.packages(..)` to install missing R packages.
_This is not required if you use the Docker container provided by the authors._

```{r install_r, eval=FALSE}
source("install.R")
```

The plots and tables of survey data and evaluation use the packages [`ggplot2`](http://ggplot2.tidyverse.org/), [`knitr::kable()`](https://yihui.name/knitr/), [`huxtable`](https://hughjonesd.github.io/huxtable/), and [`kableExtra`](https://cran.r-project.org/package=kableExtra).
Required libraries and runtime environment description are as follows.

```{r load_libraries, echo=TRUE, message=FALSE}
library("tidyverse")
library("tidyr")
library("readr")
library("ggplot2")
library("reshape2")
library("ggthemes")
library("grid")
library("gridBase")
library("gridExtra")
library("kableExtra")
library("huxtable")
library("here")
library("googlesheets4")
library("patchwork")
library("knitr")
library("kableExtra")
library("ggalluvial")
library("ggfittext")
```

## Load data

```{r evaldata_file}
assessment_file <- here::here("results/paper_assessment.csv")
codebook_file <- here::here("results/codebook.csv")
```

The data is loaded from a collaborative spreadsheet using the [`googlesheets4`](https://googlesheets4.tidyverse.org/) package.
After a year is completed by all assessors, a [named version](https://support.google.com/a/users/answer/9331169?hl=en#) of the spreadsheet is created.
Then the data file `` `r assessment_file` `` is updated from the online spreadsheet and committed to the git history.
Afterwards the individual assessments are manually merged in the online spreadsheet and the data file is updated and committed once more.

```{r googlesheets_auth, echo=FALSE, eval=FALSE}
# run once
googlesheets4::sheets_auth(use_oob = TRUE)
```

```{r evaldata_download, echo=FALSE, eval=FALSE}
assessment_sheet <- googlesheets4::read_sheet(
  ss = "https://docs.google.com/spreadsheets/d/1Df0zX5yqDuDSCzHHVaucNtThseHL5L32U2sWTPhyl0I/edit#gid=0",
  sheet = "assessment",
  # fix type for downloading data: turn all data into character strings; type set later in load
  col_types = "c",
  range = googlesheets4::cell_rows(1:88) # 2018, 2016, 2014, 2012
)

write.csv(assessment_sheet, file = assessment_file)
```

The following plots are based on the data file `` `r assessment_file` ``, the result from the manual reproducibility assessment.
The file `` `r codebook_file` `` contains a codebook with names, labels, and descriptions for the variables in the data file as documentation of the dataset outside of this computational notebook.

```{r load_evaldata}
category_levels <- c("0", "1", "2", "3")
paper_evaluation <- readr::read_csv(assessment_file, 
    col_types = readr::cols(
      `conceptual paper` = readr::col_logical(),
      `computational environment` = readr::col_factor(levels = category_levels),
      `input data` = readr::col_factor(levels = category_levels),
      `method/analysis/processing` = readr::col_factor(levels = category_levels),
      preprocessing = readr::col_factor(levels = category_levels),
      results = readr::col_factor(levels = category_levels)
      ),
    na = "NA")

paper_evaluation_wo_conceptual <- filter(paper_evaluation, `conceptual paper` == FALSE)

categoryColumns <- c("input data", 
                     "preprocessing",
                     "method/analysis/processing",
                     "computational environment",
                     "results")
```

```{r corpus_table}
options(knitr.kable.NA = '-')
knitr::kable(paper_evaluation %>% 
               dplyr::select(-matches("X1|reviewer|conceptual|authors")),
            format = "html",
            booktabs = TRUE,
            caption = paste0("Reproducibility levels for paper corpus; ",
                             "'-' is category not available")) %>%
  kableExtra::kable_styling("striped", full_width = FALSE)
```

## Table: Statistics of reproducibility levels per criterion for non-conceptual papers

```{r summary_evaldata}
evaldata_numeric <- paper_evaluation_wo_conceptual %>%
  # must convert factors to numbers to calculate the mean and median
  dplyr::mutate_if(is.factor, list(~ as.integer(as.character(.))))

# https://stackoverflow.com/questions/32011873/force-summary-to-report-the-number-of-nas-even-if-none
summaryna <- function (v) {
  if(!any(is.na(v))){
    res <- c(summary(v),"NA's"=0)
  } else{
    res <- summary(v)
  }
  return(res)
}

# apply summary independently to format as table
summaries <- sapply(evaldata_numeric[,categoryColumns], summaryna)
exclude_values_summary <- c("1st Qu.", "3rd Qu.")

kable(subset(summaries, !(rownames(summaries) %in% exclude_values_summary)), 
      digits = 1,
      col.names = c("input data", "preproc.", "method/analysis/proc.",
                    "comp. env.", "results"),
      caption = "Statistics of reproducibility levels per criterion (rounded to one decimal place)") %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling(latex_options = c("striped"),
                            font_size = 8)
```

The preprocessing has `r sum(!is.na(evaldata_numeric$preprocessing))` values, with `0` and `1` around the "middle" resulting in a fraction as the median.

## Data points for text

```{r criteria_numbers}
data_level_zero <- paper_evaluation_wo_conceptual %>% 
  filter(`input data` == 0) %>% 
  count() %>% .$n

data_level_two <- paper_evaluation_wo_conceptual %>% 
  filter(`input data` == 2) %>% 
  count() %>% .$n

preprocessing_included <- paper_evaluation_wo_conceptual %>% 
  filter(!is.na(preprocessing)) %>% 
  count() %>% .$n

preprocessing_level_one <- paper_evaluation_wo_conceptual %>% 
  filter(preprocessing == 1) %>% 
  count() %>% .$n

methods_and_results_eq_one <- paper_evaluation_wo_conceptual %>% 
  filter(`method/analysis/processing` == 1 & results == 1) %>% 
  count() %>% .$n
  
compenv_level_zero <- paper_evaluation_wo_conceptual %>% 
  filter(`computational environment` == 0) %>% 
  count() %>% .$n
```

`r data_level_zero` papers have level `0` and `r data_level_two` have level `2` in the data criterion.

`r preprocessing_included` papers include some kind of preprocessing.

`r methods_and_results_eq_one` papers have level `1` in both methods and results criterion.

## Figure: Barplots of reproducibility assessment results

```{r fig_assessment_results, fig.width=12}
# match the colours to time series plot below
colours <- RColorBrewer::brewer.pal(length(categoryColumns), "Set1")
level_names <- c("0", "1", "2", "3", NA)
breaks <- seq(from = 0, to = nrow(paper_evaluation_wo_conceptual), by = 10)

criteriaBarplot = function(category, main, colour) {
  cat <- enquo(category)
  ggplot2::ggplot(data = paper_evaluation_wo_conceptual,
                aes(!!cat),
                show.legend = FALSE) +
    ggplot2::geom_bar(fill = colours[colour], color = "black") +
    ggplot2::ggtitle(main) +
    ggplot2::xlab("Level") +
    ggplot2::xlim(level_names) +
    ggplot2::ylab("") +
    ggplot2::scale_y_continuous(breaks = breaks,
                                limits = range(breaks)) +
    ggthemes::theme_tufte(base_size = 8) + theme(axis.ticks.x = element_blank())
}

fig_barplot <- patchwork::wrap_plots(
  ncol = 5,
  criteriaBarplot(`input data`,    main = "Input data",    colour = 1),
  criteriaBarplot(`preprocessing`, main = "Preprocessing", colour = 2),
  criteriaBarplot(`method/analysis/processing`,
                  main = "Methods/Analysis/\nProcessing",  colour = 3),
  criteriaBarplot(`computational environment`,
                  main = "Computational\nEnvironment", colour = 4),
  criteriaBarplot(results,         main = "Results", colour = 5)
)

fig_barplot
```

## Figure: Alluvial diagramme of reproducibility assessment results across categories

This figure is based on package [`ggalluvial`](https://corybrunson.github.io/ggalluvial/).
It does not include the category `preprocessing` because it was discovered to be quite hard to assess, and subsequently has a large share of missing values.
The figure also does _not_ include any papers who have one or more categories as `NA`, as that means "not assessable".

```{r data_alluvial}
papers_wo_na_wo_prepr <- paper_evaluation_wo_conceptual %>%
  drop_na() %>%
  mutate_if(is.factor, forcats::fct_rev) %>%
  group_by(`input data`,
           `method/analysis/processing`,
           `computational environment`,
           results) %>%
  tally() %>%
  mutate(`Category levels (#)` = paste0(`input data`, " ",
                       `method/analysis/processing`, " ",
                       `computational environment`, " ",
                       results, " (", n, ")"))
```

```{r fig_alluvial, warning=FALSE}
fig_alluvial <- ggplot(data = papers_wo_na_wo_prepr,
       aes(axis1 = `input data`,
           axis2 = `method/analysis/processing`,
           axis3 = `computational environment`,
           axis4 = results,
           y = n)
       ) +
  ggplot2::scale_x_discrete(limits = c("Input Data",
                              "Methods/\nAnalysis/\nProcessing",
                              "Computational\nEnvironment",
                              "Results"),
                   expand = c(.1, 0)
                   ) +
  xlab("Category") +
  ylab("Number of papers") +
  ggplot2::scale_fill_manual(values = c(
    RColorBrewer::brewer.pal(9, "Set1"),            # colours from the other plots
    RColorBrewer::brewer.pal(4, "Dark2")[c(1,3,4)]) # manually chosen from another palette
    ) +
  ggalluvial::geom_alluvium(aes(fill = `Category levels (#)`), width = 1/3) +
  ggalluvial::geom_stratum(alpha = 1) +
  ggfittext::geom_fit_text(stat = "stratum", aes(label = after_stat(stratum)), min.size = 1) +
  ggthemes::theme_tufte()

fig_alluvial
```

## Table: Mean levels per criterion for non-conceptual papers

```{r summary_evaldata_grouped}
means <- lapply(evaldata_numeric %>%
                      select(all_of(categoryColumns)),
                    summary) %>%
    lapply(`[[`, "Mean") %>%
    as.data.frame()

kable(means,
      digits = 2,
      col.names = c("input data", "preproc.", "method/analysis/proc.", "comp. env.", "results"),
      caption = "Mean levels per criterion for non-conceptual papers") %>%
  kableExtra::kable_styling("striped", full_width = FALSE)
```

## Table: Mean levels averaged across criteria over time for non-conceptual papers

```{r evaldata_summary_by_year_mean}
means_years <- evaldata_numeric %>%
  filter(`conceptual paper` == FALSE) %>%
  group_by(year) %>%
  summarise(mean = mean(c(`input data`, 
                          preprocessing, 
                          `method/analysis/processing`, 
                          `computational environment`, 
                          `results`),
                        na.rm = TRUE),
            `paper count` = n())

means_years_table <- means_years %>% 
        mutate(mean = round(mean, 2), 
               `paper count` = as.character(`paper count`)) %>%
        mutate(labels = str_c(year, " (n = ", `paper count`, ")")) %>%
        column_to_rownames("labels") %>%
        select(mean) %>%
        t()

kable(means_years_table,
      caption = "Summarised mean values over all criteria over time (non-conceptual papers)") %>%
  kableExtra::kable_styling("striped", full_width = TRUE)
```

## Figure: Mean reproducibility levels per category over time for non-conceptual papers

```{r Fig4,fig.height=4,dpi=300}
evaldata_years <- evaldata_numeric %>%
  filter(`conceptual paper` == FALSE) %>%
  group_by(year) %>%
  summarise(input = mean(`input data`, na.rm = TRUE),
         preprocessing = mean(preprocessing, na.rm = TRUE),
            method = mean(`method/analysis/processing`, na.rm = TRUE),
            environment = mean(`computational environment`, na.rm = TRUE),
            results = mean(results, na.rm = TRUE))
paper_count_years <- evaldata_numeric %>%
  filter(`conceptual paper` == FALSE) %>%
  group_by(year) %>%
  summarise(`paper count` = n())

evaldata_years_long <- melt(evaldata_years, id.vars = c("year"))
fig_mean_over_time <- ggplot(evaldata_years_long, aes(year, value)) +
  geom_bar(aes(fill = variable), position = "dodge", stat = "identity") +
  #geom_errorbar(stat = "summary", fun.data = "mean_sdl", 
  #                fun.args = list(mult = 1),
  #                position =  position_dodge(width = 0.9)) +
  ylab("mean value of criterion level") + 
  scale_x_continuous(breaks = evaldata_years$year,
                     labels = paste0(paper_count_years$year, 
                                     " (n=", 
                                     paper_count_years$`paper count`, 
                                     ")")) +
  scale_fill_brewer(palette = "Set1", name = "Category") +
  ggthemes::theme_tufte(base_size = 18) +
  theme(legend.position = c(0.15,0.75), 
        legend.text = element_text(size = 14)) +
  ylim(0, 3) +
  stat_summary(fun = mean, fun.min = mean, fun.max = mean, shape = "-", size = 2) +
  stat_summary(fun = mean, geom = "line", linetype = "dotted", mapping = aes(group = 1))

fig_mean_over_time
```

## Colophon

This document is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).
All contained code is licensed under the [Apache License 2.0](https://choosealicense.com/licenses/apache-2.0/).
This document is versioned in a public [git](https://git-scm.com/) repository, [https://github.com/nuest/reproducible-research-at-giscience](https://github.com/nuest/reproducible-research-at-giscience), and archived on Zenodo at [https://doi.org/10.5281/zenodo.4032875](https://doi.org/10.5281/zenodo.4032875).

**Runtime environment description:**

```{r session_info, echo=FALSE}
options(width = 100)
devtools::session_info(include_base = TRUE)
```

```{r upload_to_drive, eval=FALSE, include=FALSE}
# upload the HTML file to the Reproducibility Committee shared folder
# upload the HTML file and source code to the Reproducibility Committee shared folder
googledrive::drive_auth(use_oob = TRUE)
googledrive::drive_put("giscience-reproducibility-assessment.html", path = googledrive::as_dribble("https://drive.google.com/drive/folders/17EUtM_zCx1gQMea1MHN_5XSVrssxv9GA"))
googledrive::drive_put("giscience-reproducibility-assessment.Rmd", path = googledrive::as_dribble("https://drive.google.com/drive/folders/17EUtM_zCx1gQMea1MHN_5XSVrssxv9GA"))
```

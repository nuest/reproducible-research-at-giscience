---
title: "Reproducible Research and GIScience: an evaluation using GIScience conference papers"
titlerunning: "Reproducible GIScience"
format: "a4paper"
hyphenation: "UKenglish"
authorcolumns: false
numberwithinsect: false # for section-numbered lemmas etc.
cleveref: true # for enabling cleveref support
autoref: true # for enabling autoref support
anonymous: false # for anonymousing the authors (e.g. for double-blind review)
thm-restate: true # for enabling thm-restate support
author:
  # mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional
  - name: Frank O. Ostermann
    footnote: Corresponding author
    affiliation: "Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, The Netherlands"
    orcid: "https://orcid.org/0000-0002-9317-8291"
    email: f.o.ostermann@utwente.nl
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Daniel Nüst
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0002-0024-5046"
    email: daniel.nuest@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant number [PE&nbsp;1632/17-1](https://gepris.dfg.de/gepris/projekt/415851837)."
  - name: Carlos Granell
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute of New Imaging Technologies, Universitat Jaume I de Castellón, Castellón, Spain"
    orcid: "https://orcid.org/0000-0003-1004-9695"
    email: carlos.granell@uji.es
    funding: "Ramon y Cajal Programme of the Spanish government, grant number RYC‐2014‐16913."
  - name: Barbara Hofer
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Interfaculty Department of Geoinformatics - Z_GIS, University of Salzburg, Salzburg, Austria"
    orcid: "https://orcid.org/0000-0001-7078-3766"
    email: barbara.hofer@sbg.ac.at
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Markus Konkol
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0001-6651-0976"
    email: m.konkol@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant numbers [KR&nbsp;3930/8-1](https://gepris.dfg.de/gepris/projekt/415851837) and [TR&nbsp;864/12-1]()."
bibliography: bibliography
authorrunning: "F. O. Ostermann, D. Nüst, C. Granell, B. Hofer, M. Konkol" # mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'
# A "thin space" character, ' ' or &thinsp;, is used between the two first names.
ccdesc:
  # Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
  concept_significance: "500"
  concept_desc: "Information systems~Geographic information systems"
keywords: "reproducible research, open science, reproducibility, GIScience"
# OPTIONAL:
acknowledgements: >
  Author contributions (see [CRediT](https://casrai.org/credit/)) FO: conceptualisation, investigation (33), writing – original draft, writing - review & editing, software; DN: conceptualisation, investigation (33), software, writing – original draft, writing - review & editing, visualisation; CG: conceptualisation, investigation (30), writing – original draft, writing - review & editing, software; BH: conceptualisation, investigation (21), writing – original draft, writing - review & editing; MK: conceptualisation, investigation (30), writing – original draft.
  The number of papers assessed by each coauthor are given in brackets after the contribution "investigation".
  We thank Celeste R. Brennecka from the Scientific Editing Service of the University of Münster for her editorial support.
#category: "Invited paper"
#relatedversion: "A full version of the paper is available at https://..." # optional, e.g. full version hosted on arXiv, HAL, or other respository/website
# optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
supplement: |
  The raw data for this work are the full texts of GIScience conference proceedings from the years 2012 to 2018 \cite{giscienceproceedings2012,giscienceproceedings2014,giscienceproceedings2016,giscienceproceedings2018}.
  The paper assessment results and source code of figures are published at [https://github.com/nuest/reproducible-research-at-giscience](github.com/nuest/reproducible-research-at-giscience) and archived on Zenodo \cite{daniel_nust_2020_4032875}.
  The used computing environment is [containerised with Docker and Binder-ready using R&nbsp;3.6.0 and Python&nbsp;3.5.3](https://github.com/rocker-org/binder/).
  The R packages are installed from the [MRAN snapshot](https://mran.microsoft.com/timemachine) of July 5th 2019, and Python packages are pinned to specific versions, see `requirements.txt` file.
nolinenumbers: true # disable line numbering
hideLIPIcs: true # remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository
# appendix _after_ the bibliography
#appendix: |
output:
  bookdown::pdf_book:
    base_format: rticles::lipics_article
    keep_tex: TRUE
header-includes:
  - \usepackage{newunicodechar}
  - \newunicodechar{^^^^2009}{\,}
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = FALSE)
```

```{r load_assessment_analysis}
# extract chunks from analysis document and read them so they can be executed individually using <<name-of-chunk>>
invisible(knitr::purl("../giscience-reproducibility-assessment.Rmd", output="temp", quiet = TRUE))
knitr::read_chunk('temp')
```

```{r run_analysis_chunks}
<<evaldata_file>>
<<load_libraries>>
<<load_evaldata>>
```

```{r count_papers, echo=FALSE, message=FALSE, warning=FALSE}
library("dplyr")

count_papers <- nrow(paper_evaluation)

count_conceptual <- nrow(paper_evaluation %>% 
                           dplyr::filter(`conceptual paper` == TRUE))
                           
count_mixed <- nrow(paper_evaluation %>% 
                      dplyr::filter(is.na(`input data`) 
                             | is.na(preprocessing) 
                             | is.na(`method/analysis/processing`) 
                             | is.na(`computational environment`) 
                             | is.na(results)))

paper_evaluation_wo_conceptual <- filter(paper_evaluation, `conceptual paper` == FALSE)

count_n <- nrow(paper_evaluation_wo_conceptual)
```

---
abstract: |
  GIScience conference authors and researchers face the same computational reproducibility challenges as authors and researchers from other disciplines who use computers to analyse data.
  Here, to assess the reproducibility of GIScience research, we apply a rubric for assessing the reproducibility of `r count_n` conference papers published at the GIScience conference series in the years 2012-2018.
  Since the rubric and process were previously applied to the publications of the AGILE conference series, this paper itself is an attempt to replicate that analysis, however going beyond the previous work by evaluating and discussing proposed measures to improve reproducibility in the specific context of the GIScience conference series.
  The results of the GIScience paper assessment are in line with previous findings:
  although descriptions of workflows and the inclusion of the data and software suffice to explain the presented work, in most published papers they do not allow a third party to reproduce the results and findings with a reasonable effort.
  We summarise and adapt previous recommendations for improving this  situation and propose the GIScience community to start a broad discussion on the reusability, quality, and openness of its research.
  The code and data for this article are published at [https://doi.org/10.5281/zenodo.4032875](https://doi.org/10.5281/zenodo.4032875).
appendix: |

  \newpage
  
  # Appendix
  
  ```{r corpus-table, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
  library("knitr")
  library("kableExtra")
  library("dplyr")
  
  knitr::kable(paper_evaluation %>%
                 select(year, title, `conceptual paper`,
                        `input data`, `preprocessing`, `method/analysis/processing`, `computational environment`, `results`), 
        longtable = TRUE,
        booktabs = TRUE,
        col.names = c("year", "title", "con-cep-tual",
                      "input data", "pre-proc.", "method/ anal./ proc.", "comp. env.", "results"),
        caption = "Assessment results excerpt; for all fields (including assessors, authors, and assessment comments) see reproducibility package at \\url{https://doi.org/10.5281/zenodo.4032875}.") %>%
    kableExtra::row_spec(0, bold = TRUE) %>%
    kableExtra::column_spec(2, width = "23em") %>%
    kableExtra::column_spec(3, width = "2.8em") %>%
    kableExtra::column_spec(4, width = "2em") %>%
    kableExtra::column_spec(5, width = "2em") %>%
    kableExtra::column_spec(6, width = "2.8em") %>%
    kableExtra::column_spec(7, width = "2em") %>%
    kableExtra::column_spec(8, width = "2.8em") %>%
    kableExtra::kable_styling(latex_options = c("striped", "repeat_header"),
                              font_size = 6)
  ```
---

# Introduction

A large proportion of GIScience research today uses software to analyse data on computers.
This means that many articles published in the context of the GIScience conference series[^giscience] fall into the categories of data science or computational research.
Thereby, these articles face challenges of transparency and reproducibility in the sense of the Claerbout/Donoho/Peng terminology \cite{barba_terminologies_2018}, where \emph{reproduction} means a recreation of the same results using the same input data and workflow as the original authors.
However, also the related concept of replication is likely to be a challenge.
So far, despite the GIScience conference series' rigorous review process, reproducibility and replicability have not been a core concern in the contributions.
With reproducibility now being a recognized topic in the call for papers, it is time to take stock and identify action. 
In previous work \cite{nust_reproducible_2018} we assessed the reproducibility of a selection of full and short papers from the AGILE conference series[^agile], a community conference organised by member labs of the Association of Geographic Information Laboratories in Europe (AGILE).
The AGILE conference is closely related to GIScience conference in terms of scientific domain and contributing authors.
Using systematic analysis based on a rubric for reproducible research, we found that the majority of AGILE papers neither provided sufficient information for a reviewer to evaluate the code and data and attempt a reproduction, nor enough material for readers to reuse or extend data or code from the analytical workflows.
This is corroborated by research in related disciplines such as quantitative geography \cite{brunsdon_quantitative_2016}, qualitative GIS \cite{muenchow_reviewing_2019}, geoscience \cite{konkol_computational_2019}, and \mbox{e-Science} \cite{freire_et_al:DR:2016:5817}.
The problems identified in these related research areas are directly transferable to GIScience, which operates at the intersections of aforementioned fields \cite{Goodchild1992}.
In any case, observations on the lack of reproducibility in all scientific fields contrast with the clear advantages and benefits of open and reproducible research both for individuals and for academia as a whole (cf.&nbsp;for example \cite{donoho_invitation_2010,markowetz_five_2015,kray_reproducible_2019,Colavizza2020}).
As a consequence, we have initiated a process to support authors in increasing reproducibility for AGILE publications; as a main outcome, this initiative has produced author guidelines as well as strategies for the AGILE conference series[^reproagile].

An obvious question is whether the GIScience conference series faces the same issues and whether similar strategies could work for improvement.
To begin this investigation, we conducted a simple text analysis of GIScience conference proceedings[^textanalysis] to evaluate the relevance of computational methods in the conference papers.
The analysis searched for several word stems related to reproducibility: Generic words, e.g., "data", "software", or "process"; specific platforms, e.g., "GitHub"; and concrete terms, e.g., words starting with "reproduc" or "replic".
Table&nbsp;\@ref(tab:wordstem-table) shows the results of the search for each year analysed.
The take-away message from the text analysis is that algorithms, processing, and data play a large role in GIScience publications, but few papers mentioned code repositories or reproduction materials.
Therefore, a more detailed manual assessment of the reproducibility of these publications was necessary.

The main contribution of this work addresses two objectives:
First, it aims to investigate the state of reproducibility in the GIScience conference community.
This investigation broadens our knowledge base about reproducibility in GIScience in general and teaches us more about the situation in the GIScience conference series specifically.
Second, it aims to apply the assessment procedure used for AGILE conference papers (presented in the section Reproducibility assessment method) to the papers of the GIScience conference, so that the broader suitability of this procedure is evaluated using a different dataset, and thereby providing evidence of its replicability.
Such a transfer validates the developed methodology.
Together, these objectives yield important findings for the discussion of reproducibility within the GIScience conference community and the GIScience discipline at large. 
We believe that GIScience as a discipline needs more studies that reproduce and replicate other studies, similar to other disciplines that have recognized the value of cross-checking and validating instead of incessant innovation (REF), and argue that such a replication study is not lacking innovation but is a prerequiste for innovating community practice.
Only then can a fruitful dialogue take place on whether and how to improve reproducibility for the GIScience conference series, and whether the recent steps taken at AGILE[^reproagile] could be an inspiration for GIScience conferences as well.
We discuss these findings and present our conclusions in the final two sections (Discussion; Conclusions and outlook).

[^giscience]: [https://www.giscience.org/](https://www.giscience.org/)
[^agile]: [https://agile-online.org/conference](https://agile-online.org/conference)
[^textanalysis]: The full text analysis and the results is available in this paper's repository in the following files: [`giscience-historic-text-analysis.Rmd`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-historic-text-analysis.Rmd) contains the analysis code; the result data are two tables with counts for occurences of words respectively word stems per year in [`results/text_analysis_topwordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_topwordstems.csv) and [`results/text_analysis_keywordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_keywordstems.csv); a wordcloud per year is in file [`results/text_analysis_wordstemclouds.png`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_wordstemclouds.png).
[^reproagile]: See the initiative website at [https://reproducible-agile.github.io/](https://reproducible-agile.github.io/), the author guidelines at [https://doi.org/10.17605/OSF.IO/CB7Z8](https://doi.org/10.17605/OSF.IO/CB7Z8) \cite{agile_guidelines} and the main OSF project with all materials [https://osf.io/phmce/](https://osf.io/phmce/) \cite{reproducible_agile}.

```{r wordstem-table, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
keywordstems_counts <- read.csv(here::here("results/text_analysis_keywordstems.csv"))

library("knitr")
library("kableExtra")

# for inline testing: kable(word_counts_sums)
knitr::kable(keywordstems_counts,
             col.names = c("year", "words",
                           "reproduc..",
                           "replic..",
                           "repeatab..",
                           "code",
                           "software",
                           "algorithm(s)",
                           "(pre)process..",
                           "data.*",
                           "result(s)",
                           "repository/ies",
                           "github/lab"),
             booktabs = TRUE,
             caption = "Reproducibility-related word stems in the corpus per year of proceedings") %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling(latex_options = c("striped", "scale_down"),
                            font_size = 6) %>%
  kableExtra::footnote(general = "The very high value for 'code' in 2012 is due to a single paper about land use, for which different \"land use codes\" are defined, discussed and used.",
                       title_format = c("italic"))
```

# Related work

This work transfers a previously used method to a new dataset, whereby the original article \cite{nust_reproducible_2018} already provides an overview of reproducible research in general, including definitions, challenges, and shortcomings.
In the following, we focus on recently published works and briefly introduce related meta-studies.

Few groups have attempted practical reproduction of computational works related to GIScience.
Konkol et&nbsp;al.&nbsp;\cite{konkol_computational_2019} conducted an in-depth examination of the computational reproducibility of 41 geoscience papers with a focus on differences between the recreated figures.
The set of papers was, similar to our work, drawn from a fixed group of two outlets (journals), but it was further limited to recent papers providing code in the R language.
Konkol et&nbsp;al.&nbsp;\cite{konkol_computational_2019} also conducted actual reproductions, which we did not do here; yet, applying the same prerequisites to our dataset would be possible as a next step.
One could attempt to reproduce the papers whose assessment points to a possible reproduction (i.e., level two or higher, see below).
In a report on the reproducibility review at the AGILE conference 2020[^reproreport], the reproducibility committee summarised the process and documented relevant obstacles to reproducibility of accepted papers.
The main issues identified in their report and by Konkol et&nbsp;al.&nbsp;\cite{konkol_computational_2019} are quite coincident.

Within the geospatial domain, Kedron et&nbsp;al.&nbsp;\cite{kedron_reproducibility_2020} provide a recent review of opportunities and challenges for reproducibility and replicability.
They transfer solutions from other domains but also discuss and conceptualize the specific nature of a reproducibility and replicability framework when working with geospatial data, e.g., handling context, uncertainty of spatial processes, or how to accommodate the inherent natural variability of geospatial systems.
In a similar manner, Brunsdon and Comber \cite{brunsdon_opening_2020} investigate reproducibility within spatial data science, with special attention to big spatial data.
They support the need for open tools, knowledge about code, and reproducibility editors at domain journals and conferences, but they also introduce the perspective that spatial analysis is no longer conducted only by GI/geo-scientists or geographers and connect reproducibility with critical spatial understanding.
The conceptual work in these articles complements the assessment of reproducibility conducted in this paper.

Two recent studies from distant disciplines, wildlife science \cite{archmiller_computational_2020} and hydrology \cite{stagge_assessing_2019}, relate to our work in this paper.
Both investigate a random set of articles from selected journals and use a stepwise process of questions to determine the availability of materials, and eventually reproduce workflows if possible.
Archimiller et&nbsp;al.&nbsp;\cite{archmiller_computational_2020} use a final ranking of 1 to 5 to specify the degree to which a study's conclusions were eventually reproduced.
Similar to our classification scheme, their ranking models borrow the general notion of a _"reproducibility spectrum"_ \cite{peng_reproducible_2011}.

[^reproreport]: [https://osf.io/7rjpe/](https://osf.io/7rjpe/)

# Reproducibility assessment method

## Criteria

The assessment criteria used for the current study were originally defined in previous work, so we provide a short introduction here and refer to Nüst et&nbsp;al.&nbsp;\cite{nust_reproducible_2018} for details.
The three assessment criteria are _Input Data_, _Methods_, and _Results_.
_Input Data_ comprises all datasets that the computational analysis uses.
_Methods_ encompasses the entire computational analysis that generates the results.
Since _Methods_ is difficult to evaluate as a whole, we split this criterion into three subcategories:
_Preprocessing_ includes the steps to prepare the _Input Data_ before the main analysis;
_Methods, Analysis, Processing_ is the actual analysis; 
_Computational Environment_ addresses the description of hard- and software.
Finally, the criterion _Results_ refers to the output of analysis, for example, figures, tables, and numbers. 

For each of these (sub)categories, we assigned one of four levels unless the criterion was not applicable, _(Level NA)_.
_(Level 0) Unavailable_ means that it was not possible to access the paper’s data, methods, or results, and that it was impossible to recreate them based on the description in the paper.
_(Level 1) Documented_ indicates that the paper still did not provide direct access to datasets, methods, or results, but that there was sufficient description or metadata to recreate them closely enough for an evaluation; yet, often a recreation was unlikely due to the huge amount of effort needed.
With regard to the methods criteria, _Level 1_ means that pseudo code or a textual workflow description was available.
_(Level 2) Available_ was assigned if the paper provided direct access to the materials (e.g., through a link to a personal or institutional website), but not in the form of an open and permanent identifier, such as a DOI.
The indication of a DOI does not apply to the methods criteria, as it is not yet common practice to make a permanent reference to code, libraries and system environments with a single identifier.
The gold standard, _(Level 3) Available and open_, requires open and permanent access to the materials (e.g., through public online repositories) and open licenses to allow use and extension.

Note that levels are ordinal numbers that can be compared (3 is higher than 2), but absolute differences between numbers must not be interpreted as equals.
Moving one level up from 0 to 1 is not the same as from 1 to 2.
While reaching 1 is fairly straightforward, jumping to level 2 means one must have an almost fully reproducible paper.

## Process

The overall approach to assessing the reproducibility of GIScience papers, again, followed the previous assessment of AGILE papers \cite{nust_reproducible_2018}.
The assessment was conducted by the same persons.
All full papers in the GIScience conference series (from the 2012 to 2018 editions) were assessed.
This is partly because of no obvious subset, such as the nominees for best papers as in the case of the AGILE conference series; this was also partly because we aimed to work with a larger dataset for potentially more informative results.
Each GIScience paper was randomly assigned to two assessors who evaluated it qualitatively according to the five reproducibility criteria.
The assessors were free in the way they approached the assigned evaluations.
The particular process depended on the structure of the paper and the assessor's familiarity with the topic, and it could range from a quick browse to identify relevant statements to a thorough reading of the full text.
The identification of relevant content could be supported to some extent by a PDF reader with multiple highlights, using keywords like e.g., "data, software, code, download, contribution, script, workflow".
The results of the individual assessments were joined in a collaborative Google Spreadsheet.
This spreadsheet also had a comments column for assessors to record relevant sources and decisions.
When there was disagreement between assessors, arguments for and against a certain reproducibility level were discussed in the entire group of five assessors until a consensus was reached.
Only then were the assessments merged into a single value, one year at a time.
A snapshot of both the unmerged and merged values was stored as a CSV file in the collaboration repository for transparency and provenance[^assessmenttable].
Two independent assessors per paper increased the objectivity of the final assessment.
Disagreements and conducting the assessment one year at a time, going backwards from the most recent year, were found helpful in aligning the interpretation of criteria and, in rare cases, led to an adjustment of similar cases in other papers.

The discussion about the correct assignment of levels made us reflect on how to apply the rubric for special situations.
For the _Input Data_ criterion, some papers had input data "available" at the time of writing/publication but it was not available at the time of evaluation, due to broken links, changes in the URL structure of a website, or projects and/or personal websites that were down or moved.
In such cases, we gave the authors the benefit of the doubt and assumed the data were accessible some time after the publication of the conference proceedings.
We did not give those papers an arbitrary score and discussed internally the best level per case; yet, such papers never earned a `3`, which would include permanent resolving of the link.
Related to this criterion, simulation data, like the specification or configuration of agents in an agent-based system, was not treated as input data (_Level NA_), but as parameters of the main analysis, i.e., being part of the _Methods, Analysis, Processing_.

_Preprocessing_ covers preparatory work for the actual analysis involving various tasks such as data selection, cleaning, aggregation and integration.
However, the dividing line between data preprocessing and processing (i.e., the main analysis) is often thin and not always clearly defined.
Several times, assessors' opinions were not in agreement if the preprocessing criterion was "not applicable" or "applicable"; if it was applicable, there was disagreement on whether it was not reproducible at all or hardly reproducible (`0` or `1`, respectively).
Therefore, we decided to apply the rubric only in cases where papers themselves specifically mentioned a preprocessing task independent of the actual analysis or method, e.g., when clearly stated in separate sub-sections of the paper.

Lastly, human subject tests and surveys were also a special case. Human-related research activities were rated as `1` in the methods/analysis/processing criterion if sufficiently documented; nonetheless, a sufficient documentation in these cases did not mean that original sources were available or could be exactly recreated. 

[^assessmenttable]: The assessment results are in the file [`results/paper_assessment.csv`](). As an example, commit [`464e630`](https://github.com/nuest/reproducible-research-at-giscience/commit/464e63003a550db216c827571e503d464615efc7) and  [`2e8b1be`](https://github.com/nuest/reproducible-research-at-giscience/commit/2e8b1bebd6298c5ccfd61af1f67a75c082136d3e) are the pre-merge and post-merge commit after completing the assessment of the papers from 2014. The pre-merge commit contains the assessments including the assessors' initials, e.g. "CG: 1, MK: 1".

## Paper corpus

```{r years}
conceptual_counts_per_year <- paper_evaluation %>%
  dplyr::filter(`conceptual paper` == TRUE) %>%
  dplyr::group_by(year) %>%
  dplyr::summarise(n = n()) %>%
  tidyr::unite(counts, year, n, sep = ":&nbsp;")
    
years <- sort(unique(paper_evaluation$year))
```

In total, `r count_papers` papers from the GIScience conferences in `r paste0(paste(sort(years[1:length(years)-1]), collapse = ", "), ", and ", years[length(years)])` were assessed.
Table&nbsp;\@ref(tab:corpus-table) shows the full results of the assessment.
For details on assigned assessors, authors, etc., please check the reproducibility package \cite{daniel_nust_2020_4032875}.
`r count_conceptual` papers (`r paste0(round(count_conceptual / count_papers * 100, 0), "%")`) across all years were identified as conceptual papers[^conceptual] and were not included in the corpus.
The number of conceptual papers in GIScience conferences was low over the analysed years (`r paste(conceptual_counts_per_year$counts, collapse = "; ")`), going down to zero in the last included year's proceedings (`r max(unique(paper_evaluation$year))`).
This might suggest an increasingly predominant and ubiquitous role of analytics datasets and computational workflows in the generation of the final published results in the field.

# Reproducibility of GIScience conference papers

Table&nbsp;\@ref(tab:summary-evaldata) shows aggregated values for the assessed reproducibility levels. 
For `r paper_evaluation_wo_conceptual %>% filter(is.na(preprocessing)) %>% count() %>% .$n` papers, the _Preprocessing_ criterion was not applicable.
This large number can be explained with the unclear boundary between the _Preprocessing_ criterion and the _Methods, Analysis, Processing_ criterion.
Therefore, especially the _Preprocessing_ criterion was found to be subject to the assessor's interpretation.
This does not mean that the papers in which the _Preprocessing_ criterion was missing are not reproducible at all.
Simply, in these cases, either no preprocessing is required or it has been integrated into the main analysis.
Obviously, if data preprocessing is required but it is either not indicated in the paper or is not provided as an additional (computational) step or resource, the ability to reproduce the paper will be seriously limited.

```{r summary-evaldata, ref.label="summary_evaldata", out.width="100%"}
```

If we look at the median values of the five criteria (Table&nbsp;\@ref(tab:summary-evaldata)), a typical GIScience paper scores `1`&nbsp;`1`&nbsp;`1`&nbsp;`0`&nbsp;`1`.
This score translates in practical terms into a paper that is sufficiently documented to claim that reproduction could be attempted within a short time frame after publication.
While such a level of reproducibility is typically accepted by journals and conferences today, it does not guarantee that a reproduction would be possible and practical.
A reproduction of such a paper would require considerable effort, namely technical skills, communication with authors, and time not only to both gather, recreate, and/or analyse all the necessary resources (data, code, etc.) but also to recreate the specific computational environment of the paper.
Especially the latter is very unlikely, as the computational environment is generally not specified at all (note the lone value `0` in the fourth position above).

```{r ref.label="criteria_numbers"}
```

Figure&nbsp;\@ref(fig:barplot) shows the distribution of the reproducibility levels for each criterion.
None of the papers reached the highest level of reproducibility (`3`) on any criterion.
Only `r data_level_two` papers reached level `2` in the _Input Data_ criterion.
Similar to previous results \cite{nust_reproducible_2018}, the number of papers with level `0` for _Input Data_ was especially high (`r data_level_zero`, corresponding to `r paste0(round(data_level_zero / count_n * 100, 0), "%")`), which is a significant barrier to reproduction since input data is not only unavailable but also cannot be recreated from the information provided in the paper.

```{r barplot, ref.label="fig_assessment_results", out.width='100%', fig.cap="Barplots of reproducibility assessment results; levels range from 0 (leftmost bar) to 'not applicable' (rightmost bar)."}
```

_Preprocessing_ applied to `r preprocessing_included` publications, and the levels reached were generally low (`0` or `1`), level `2` being residual.
Of these, `r preprocessing_level_one` papers reach level `1`.
This represents about half of the papers with level `1` in the _Analysis_ criterion, suggesting that data processing was clearly identified in some papers.
For the other half, it was not clear whether data preprocessing tasks existed at all, or whether these tasks were part of the main analysis.
When data preprocessing steps are required but not reported, this situation becomes much more problematic, as this makes it noticeably difficult to reproduce the results of a paper.

_Methods_ and _Results_ criteria show a pretty similar distribution (see Figure&nbsp;\@ref(fig:assessment-results)).
Indeed, `r methods_and_results_eq_one` publications had level `1` in both criteria, which represents `r paste0(round(methods_and_results_eq_one / count_n * 100, 0), "%")` of the papers assessed.
In this sense, most of the assessed papers fall below the minimum standard for reproduction as regards the methods and results criteria. 
All papers except one had a value of `1` for the results criterion, which shows that the peer review worked as expected for almost all articles.
In other words, authors are concerned with making the results understandable to the reviewers, which is not always the case for the other criteria.
More generally, this aspect raises the question of whether peer review should stop in the absence of minimal evidence of the input data, analysis, and computational environment used in a paper.
Without an explicit justification (confidential data, privacy concerns, etc), should a paper with level `0` (or `NA`) for the input data criterion get accepted for a conference?

Finally, papers scored worse on the _Computational Environment_ criterion.
Overall, `r compenv_level_zero` (`r paste0(round(compenv_level_zero / count_n * 100, 0), "%")`) publications were level `0`, which means that no clues were provided in the paper about the computing environment, tools, or libraries used in the reported analysis.
The _Computational Environment_ criterion and the _Input Data_ criterion accounted for a considerable number of `0` values, which clearly signals a serious impediment to reproduction.
It also shows a rather low recognition of data and software as academic outputs, because both data and software should be properly cited to give credit to their creators \cite{lawrence_bryan_citation_2011,katz_recognizing_2021}.

```{r data-alluvial, ref.label='data_alluvial'}
```

Figure&nbsp;\@ref(fig:alluvial-diagramme) shows an alluvial diagramme of all scores, i.e., combinations of category values, of `r sum(papers_wo_na_wo_prepr$n)` papers, which do not have any categories with level `NA`.
A large part of the not included papers have this value within the category `preprocessing` because it was discovered to be quite hard to assess.
Therefore this category is not shown in the figure.
The diagramme repeats overall patterns seen before.
The vast majority of papers have level `1` in categories in _Methods/Analysis/Processing_ and _Results_.
_Input data_ is most diverse, with a surprisingly large number of papers with level `0` but also the largest fraction of papers reaching level `2`.
All kinds of papers show considerable drops in _Computational Environment_.

```{r alluvial-diagramme, ref.label='fig_alluvial', echo=FALSE, message=FALSE, warning=FALSE, out.width='100%', fig.cap="Alluvial diagramme of common groups of papers throughout 4 of 5 categories including only papers without any ``not applicable'' \\emph{(Level NA)} value; category \\emph{Preprocessing} was dropped because difficulty to clearly assess it lead to many ``not applicable'' values."}
```

More interestingly, the diagramme illustrates how groups of papers with the same properties move through the different categories.
Three major groups, who represent `r papers_wo_na_wo_prepr %>% filter(n > 9) %>% .$n %>% sum()` (`r round(papers_wo_na_wo_prepr %>% filter(n > 9) %>% .$n %>% sum() / sum(papers_wo_na_wo_prepr$n), 2) * 100`%) of the papers included in the figure, become visible as broad bands.
These papers only get close to meet the minimum standard for reproducibility, and provide no assurance at all that the level can be kept in the mid term as materials are not properly deposited.
Two groups with 10 papers each start with level `0` for _Input Data_ and `1` for _Methods/Analysis/Processing_ and reach a `1` for _Results_, while they are divided equally between level `0` and `1` for _Computational Environment_.
These groups seriously question the focus of the reporting and arguments in the papers, as convincing results seem to outweigh all concerns for transparency and full understandability.
The third and largest group matches the overall mean values for the typical GIScience paper with level `1` for all categories except for _Computational Environment_.

Most interestingly, the diagramme shows some more curious patterns albeit with only few papers.
The papers with the lowest level of `0` in _Results_, so in the eyes' of the assessors the results are not fully understandable, actually have better values in earlier categories.
Only few papers that start with level `2` in _Input Data_ can keep this level for _Methods/Analysis/Processing_, and even the ones who do then drop to level `0` in _Computational Environment_.
Only one paper each shows the following surprising paths: a start with level `1` for _Input Data_ and then a move up to level `2` in the methodology; reaching level `2` in results despite having only values of `1` or `0` in other categories.

Not a single paper can reach the required levels for an immediate reproduction by ensuring that all required pieces are available (levels of `2`), not even considering the further challenges around documentation if availability is given (e.g.,&nbsp;\cite{nust_improving_2020}).
The overall low values for _Computational Environment_ are one signal that lead to the growing concerns for reproducibility and reusability of computational research.

[^conceptual]: See \cite{nust_reproducible_2018} for a definition of "conceptual".

# Discussion

## State of reproducibility in the GIScience conference series

Our first research objective was to assess the state of reproducibility in the GIScience conference series.
A recurrent issue found in the analysis was the inability to access input data based on the information provided in the paper.
Most of the links and pointers to datasets reported at the time of publication were either broken (e.g., non-existing resource, HTTP 404 error, invalid URL syntax) or not available anymore (URL works but redirects to a different generic page; specific resource from the paper no longer exists).
In these cases, a level of `2` in the _Input Data_ criterion was deserved at the time of publication; however, when evaluating the level of reproducibility some time later, as was done in this work, level `2` is no longer suitable for those papers.
From the reproducibility point of view, the input data was not accessible, even if contacting the authors could still be attempted.
According to the meaning of the criterion and in practical terms, this is equivalent to including the statement "available upon request" in the paper and thereby level `0`.
An important part of reproducibility is that access to material should not degrade over time, which is best achieved by depositing data in repositories, including sensitive data (using the appropriate mechanisms), and properly citing it.
In this assessment of reproducibility, we decided to give the authors the benefit of the doubt and awarded a value of `2` for _Input Data_ even if we could not conclusively determine, e.g., by using the Internet Archive's Wayback Machine[^wayback], whether the original website ever existed.

[^wayback]: [https://web.archive.org/](https://web.archive.org/).

Regarding the common situation of a paper with _Documented_ (level `1`) for all criteria, our interpretation is that this is indeed a regular paper that is up to current scientific standards.
Even an occasional _Unavailable_ (level `0`) in some criteria does not mean the paper should not have passed peer review - after all, we specifically assessed a paper's reproducibility using criteria, which likely were not included at all in the call for papers or in the reviewer guidelines, and therefore did not receive much attention from authors or reviewers.
Thus, we are analysing in the historical context of when there were no concrete incentives to push these aspects, beyond the general concerns for good scientific practice.

However, this overall picture matches the somewhat worrisome focus that researchers, editors, and publishers have on publishing exciting results in science, and not giving as much care to publishing the full process and collection of parts that would allow readers to try to fully understand the research. 
Clearly, a level of `0` in _Input Data_ is problematic, because without sufficient knowledge about the characteristics of the input data, all attempts at reproducing results are bound to fail, even when the textual documentation of the data would potentially allow for an expensive recreation of the computational workflow.

## Transferability of method

Concerning our second research objective, we can state that the overall process and the application of the reproducibility rubric was successfully replicated with the different data set.
This is not entirely surprising given that AGILE and GIScience conference series share similarities in target audience, review process, and publication of proceedings (more on that in the following section).
More importantly, the process faced similar challenges as we recalled from its earlier application.
This is very important information, because the successful replication of the process, including its challenges, enables us and others to ground any changes in solid evidence.
In particular the preprocessing criterion caused many discussions among the reproducibility reviewers during the assessment.
It is often not clear or a matter of interpretation if a particular processing step belongs to a minor basic transformation of input data, if it is already part of the main analysis, and when it is a truly distinct step in the process.
The borders are vague and hence scores should be interpreted with caution.
Future assessments could provide a more precise definition for _pre_-processing, e.g., only use it if the authors use the term, or might consider to drop the category.

In a similar vein, the computational environment is difficult to clearly distinguish from analysis, and technology and practices for the effective management of the computing environment have reached mature states relatively recently.
Future assessments could prepare concrete rules as reliefs for historic workflows, similar as discussed for input data above.

Furthermore, it is important to remember that the levels of reproducibility are not equidistant in the sense that a level of `2` would be twice as good as a level of `1`, or that the effort needed is twice as high.
A level of 1 should be the standard for peer-reviewed papers.
Moving it to `2` requires several steps and actions, while reaching the gold standard of `3` is a comparatively small step from level `2` - the main difference is to use public repositories with a DOI - but with a comparatively positive impact in permanent accessibility.

## Comparison of conferences

```{r agile_values_for_comparison, echo=FALSE, warning=FALSE}
agile_input_data_mean <- 0.67
agile_method_mean <- 1.0
agile_compenv_mean <- 0.62
agile_results_mean <- 0.88

agile_papers <- 32
agile_conceptual <- 5
agile_n <- agile_papers - agile_conceptual
```

Given that we followed the same process as in \cite{nust_reproducible_2018} and demonstrated the transferability of the method, comparing the two conference series seems appropriate.
It is important to remember that we do not attempt such a comparison with the objective of declaring a "winner".
The two conferences are similar enough, as shown below, for a comparison, yet they are too distinct and diverse in setup, process, and audience for such a simplistic ranking.
However, a comparison is required to sensibly discuss whether the guidelines developed for AGILE might also be promising for GIScience:
Are they transferable?
If not, what adaptations seem necessary?

```{r author_analysis, echo=FALSE, message=FALSE, warning=FALSE}
author_counts <- read.csv(here::here("author_analysis", "author_counts.csv"))
```

Concerning the contributing and participating academic communities, Egenhofer et&nbsp;al.&nbsp;\cite{egenhofer_contributions_2016} and Kemp et&nbsp;al.&nbsp;\cite{kemp2013results} both include  the conferences considered here as outlets for GIScience.
Further, Ke\ss ler et&nbsp;al.&nbsp;\cite{kesler_spatiallinkedscience_2012} investigate the bibliographies of four GIScience conference series, including GIScience and AGILE.
They list 15 authors who have published in all GIScience conference.
We conducted a cursory investigation of the body of authors for full papers, revealing significant overlap[^authoranalysis]: Out of `r author_counts$AGILE` unique AGILE and `r author_counts$GIScience` unique GIScience full paper authors, `r author_counts$intersection` published in both conferences, and this includes all 15 authors mentioned by Ke\ss ler et&nbsp;al.&nbsp;\cite{kesler_spatiallinkedscience_2012}. 
Therefore, the strong relation between the AGILE and GIScience conference series confirms our approach to apply the same methodology to GIScience that has been developed for AGILE conference publications, and it might lead to similar implications for improving reproducibility.

Concerning the paper corpora, the publication years considered here (2012-2018) are similar to the assessment of AGILE papers (2010-2017), which makes the results comparable in the sense of what methods and tools would have been available for authors.
Furthermore, we note that both conferences have a similar ratio of conceptual papers which were not assessed for reproducibility:
In the AGILE corpus we identified `r agile_conceptual` of `r agile_papers` conceptual papers (`r round(agile_conceptual / agile_papers * 100, 1)`%), in the GIScience corpus there were `r count_conceptual` of `r count_papers` (`r round(count_conceptual / count_papers * 100, 1)`%).
This indicates that both conferences have similar share of papers that used, at least in part, computational methods.
On the content of the papers, our overall impression was that a larger share of GIScience papers included theoretical, conceptual, or methodological aspects, while AGILE papers seemed to feature more empirical and/or applied geoinformation science research.

```{r comparison-table}
criteria <- c("input data", "method/analysis/processing", "computational environment", "results")
comparison_table <- tibble(Criterion = criteria,
                           `AGILE full papers` = c(agile_input_data_mean,
                                                   agile_method_mean,
                                                   agile_compenv_mean,
                                                   agile_results_mean),
                           `GIScience papers` = summaries["Mean", criteria])

knitr::kable(comparison_table,
      digits = 2,
      caption = "Mean values per criterion for both conferences (rounded to two decimal places)") %>%
  kableExtra::row_spec(0, bold = TRUE) %>%
  kableExtra::kable_styling(latex_options = c("striped"),
                            font_size = 8)
```

Regarding the results of the reproducibility assessments as summarised in Table&nbsp;\@ref(tab:comparison-table), the nature of the data and sample size does not support statistical analyses on significant differences.
Nevertheless, looking at the _Input Data_ category, GIScience has a slightly higher mean value compared to AGILE full papers (`r round(summaries["Mean","input data"], 2)` as opposed to `r agile_input_data_mean`) and a median of `r round(summaries["Median","input data"], 0)`.
These values indicate that the GIScience contributions had a slightly better, but by no means optimal, availability of input data.
The pattern of reproducibility of the papers' workflows (category _Method, Analysis, Processing_) was very similar for the two conference series:
The overwhelming majority of papers achieved a level of `1`, resulting in a mean of `r round(summaries["Mean","method/analysis/processing"], 2)` for GIScience and `r agile_method_mean` for AGILE full papers.
The _Computational Environment_ category was not that different either: AGILE scored better with a mean of `r agile_compenv_mean` vs. `r round(summaries["Mean","computational environment"], 2)` for GIScience.
The _Results_ category scores were again slightly higher for GIScience, with a mean of `r round(summaries["Mean","results"], 2)` vs. a mean of `r agile_results_mean` for AGILE.
Several papers in AGILE received a level of `0` here, indicating that crucial information is missing to connect analysis outputs and presented results.
We refrain from comparing the _Preprocessing_ category, because our analysis has shown that this is a somewhat contentious dimension among assessors.

Before discussing any strategies to improve reproducibility, it is important to identify and consider the differences between the two conference series.
GIScience is a biannual conference series whereas AGILE is annual, and they feature different pre-publication review processes and review management systems:
In AGILE both authors and reviewers are anonymous, while in GIScience only the reviewers are.
Furthermore, the AGILE conference series has the AGILE association[^agileorg] as an institutional supporter, which means a more stable organizational and financial framework for activities spanning more than one or between conferences.
However, like GIScience, local conference organizers for AGILE have  the main financial burden and experiences are informally handed over between organizing committees.
Geographic focus is also different:
GIScience has a global target audience and the individual conferences are likely to be different in their contributor communities because of the moving conference location, which often means lowered accessibility for other parts of the world.
AGILE, by comparison, has a European focus, and, although the conference location moves every year, accessibility is less diverse. 
This likely translates into a less fluctuating and less geographically diverse audience at AGILE.

This comparison lets us draw two main conclusions.
First, we conclude that both the target audience and the content of the two conference series are similar enough to be afflicted with similar shortcomings in terms of reproducibility, and, thus, they both likely respond to similar solutions.
Second, we conclude that the AGILE conference series seems structurally better positioned to support changing habits, because of a more stable audience and institutional support.
The introduction of the AGILE reproducibility guidelines was done within a short time frame and with financial support in the form of an "AGILE initiative", including travel funding for an in-person workshop.
For GIScience, the task of changing the review process to foster better reproducibility falls squarely on the shoulders of the changing program committees. 
However, the initial results of AGILE's new guidelines show that even small changes can lead to a significantly improved outcome.

[^agileorg]: [https://agile-online.org/](https://agile-online.org/).
[^authoranalysis]: The data and code for the brief exploration into the authorships across the conferences considered in this work can be found in the directory `author_analysis` of this paper's reproducibility package \cite{daniel_nust_2020_4032875}.

# Conclusions and outlook

In this work we investigated the reproducibility of several years of GIScience conference publications.
The paper corpus is large enough for a representative sample.
The corpus size is comparable to that used for the AGILE assessment study.
The corpora have different but largely overlapping time windows.
It was never the intention of this study to rate the papers or to compare AGILE vs. GIScience conference quality.
We also do not question that the research presented in these papers is sound and relevant, since they were accepted for publication at a reputable conference.
Instead, we investigated the papers along a single desirable quality dimension, reproducibility, which implies requirements on openness and transparency.

Using a similarly high bar for reproducibility as in the earlier assessment study, the results clearly show a lot of room for improvement, as none of the presented articles were readily reproducible.
The majority of articles provided some information, but not to the degree required to facilitate transparent, and, most relevantly, reusable research based on data and software.
Overall, this is very similar to the outcomes of our earlier study on AGILE papers.
As part of the AGILE assessment, we described concrete recommendations for individuals and organizations to improve paper reproducibility \cite{nust_reproducible_2018}.
We have argued that AGILE and GIScience share a sufficiently common starting position, domain/discipline characteristics, audience, and author community, such that for both communities the strategies to improve the situation should be similar.
Therefore, the previously identified recommendations are directly transferable to the GIScience conference series, the most important recommendations being (1) promoting outstanding reproducible work, e.g., with awards or badges,
(2) recognizing researchers' efforts to achieve reproducibility, e.g., with a special track for reproducible papers, implementing a reproducibility reviewer, open educational resources, and helpful author guidelines including data and software citation requirements and a specific data/software repository, and
(3) making an institutional commitment to a policy shift that goes beyond mere accessibility \cite{stodden_empirical_2018}.
These changes require a roadmap and a clear year, say 2024, when GIScience starts to only accept computationally reproducible submissions and to check reproducibility before papers are accepted.
The concluding statement of Archmiller et&nbsp;al.&nbsp;\cite{archmiller_computational_2020} is directly transferable to GIScience:
The challenges are not insurmountable, and increased reproducibility will ensure scientific integrity.

The AGILE reproducible paper guidelines \cite{agile_guidelines} and the associated reproducibility review processes as well as other community code review systems such as CODECHECK \cite{eglen_codecheck_2019} are open and "ready to use".
They can also be adopted for GIScience conferences, e.g., to suit the peer review process goals and scheduling.
Kedron et&nbsp;al.&nbsp;\cite{kedron_reproducibility_2020} stressed the need for a comprehensive balanced approach to technical, conceptual, and practical issues.
They further pointed out that availability must not lead to adoption.
Therefore, a broad discourse around these recommendations, tools, and concepts would be beneficial for all members of the community, whether their work is more towards conceptual, computational, or applied GIScience.
A survey for authors, as conducted for AGILE \cite{nust_reproducible_2018}, could help identify special requirements and specific circumstances, beyond the findings presented here and in related work.

Future work may transfer the assessment process to other major events and outlets for GIScience research, such as GeoComputation or COSIT conferences and domain journals (cf.&nbsp;\cite{egenhofer_contributions_2016} for an extensive list), but we would not expect significantly differing results.
Practical reproductions of papers and even replications of fundamental works are even more promising projects to convincingly underpin a call for a culture change.
For example, Egenhofer et&nbsp;al.&nbsp;\cite{egenhofer_contributions_2016} provide for a list of the most frequently cited articles as potential candidates.
Such a project would ideally be supported with proper funding.
If the observed trend of non-reproducibility is continued in further, or in the case of replications, much more detailed evaluations, which we fear it might, the GIScience community should take action and improve its state of reproducibility.
A timely adoption of the technological and procedural solutions may allow GIScience researchers, together with the entirety of academia, to level up and approach the challenges of the _"second phase of reproducible research"_ by tackling long-term funding for maintenance of code and data and building supporting infrastructure for reproducible research \cite{peng_reproducible_2020}.

```{r wordcount}
message("WORDCOUNT: ", wordcountaddin::word_count())
```

```{r}
unlink("temp")
```

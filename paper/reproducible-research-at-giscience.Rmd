---
title: "Reproducible Research and GIScience: an evaluation using GIScience conference papers"
titlerunning: "Reproducible GIScience"
format: "a4paper"
hyphenation: "UKenglish"
authorcolumns: false
numberwithinsect: false # for section-numbered lemmas etc.
cleveref: true # for enabling cleveref support
autoref: true # for enabling autoref support
anonymous: false # for anonymousing the authors (e.g. for double-blind review)
thm-restate: true # for enabling thm-restate support
author:
  # mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty. Please provide at least the name of the affiliation and the country. The full address is optional
  - name: Barbara Hofer
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Interfaculty Department of Geoinformatics - Z_GIS, University of Salzburg, Salzburg, Austria"
    orcid: "https://orcid.org/0000-0001-7078-3766"
    email: barbara.hofer@sbg.ac.at
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Carlos Granell
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute of New Imaging Technologies, Universitat Jaume I de Castellón, Castellón, Spain"
    orcid: "https://orcid.org/0000-0003-1004-9695"
    email: carlos.granell@uji.es
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Daniel Nüst
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0002-0024-5046"
    email: daniel.nuest@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant number [PE&nbsp;1632/17-1](https://gepris.dfg.de/gepris/projekt/415851837)."
  - name: Frank Ostermann
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, Enschede, The Netherlands"
    orcid: "https://orcid.org/0000-0002-9317-8291"
    email: f.o.ostermann@utwente.nl
    #funding: "(Optional) author-specific funding acknowledgements"
  - name: Markus Konkol
    #footnote: Optional footnote, e.g. to mark corresponding author
    affiliation: "Institute for Geoinformatics, University of Münster, Münster, Germany"
    orcid: "https://orcid.org/0000-0001-6651-0976"
    email: m.konkol@uni-muenster.de
    funding: "Project [_o2r_](https://www.uni-muenster.de/forschungaz/project/12343), German Research Foundation, grant number [KR&nbsp;3930/8-1](https://gepris.dfg.de/gepris/projekt/415851837)."
abstract: |
  Authors of papers at the GIScience conference face challenges of computational reproducibility just like any other discipline using computers to analyse data.
  In this work, we apply a rubric for assessing the reproducibility to XX conference papers published at the GIScience conference series in years 20XX-2018.
  The rubric was originally developed for an assessment of publications at the AGILE conference.
  The results of the GIScience paper assessment are in line with previous findings.
  The description of workflows and publication of data and software used in most papers suffice to explain the presented work, but are far from enabling a reproduction by a third party with reasonable effort.
  We summarise and adapt previous recommendations for improving this dire picture and invite the GIScience community to start a broad discussion on reusability, quality, and openness of its research.
bibliography: bibliography
authorrunning: "TODO authorrunning when order fixed" # mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'
# A "thin space" character, ' ' or &thinsp;, is used between the two first names.
copyright: "TODO copyright when order fixed, John Q. Public and Joan R. Public" # mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/
ccdesc:
  # Please choose ACM 2012 classifications from https://dl.acm.org/ccs/ccs_flat.cfm
  concept_significance: "500"
  concept_desc: "Information systems~Geographic information systems"
keywords: "reproducible research, open science, reproducibility, GIScience"
# OPTIONAL:
#acknowledgements: "I want to thank \\dots"
#category: "Invited paper"
#relatedversion: "A full version of the paper is available at https://..." # optional, e.g. full version hosted on arXiv, HAL, or other respository/website
# optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...
supplement: |
  The raw data for this work are the full texts of conference GIScience conference proceedings \cite{giscienceproceedings2018,giscienceproceedings2016}.
  The paper assessment results and source code of figures are published at [https://github.com/nuest/reproducible-research-at-giscience](github.com/nuest/reproducible-research-at-giscience) and archived at TODO ZENODO URL.
  The used computing environment is [containerised with Docker and Binder-ready using R&nbsp;3.6.0](https://github.com/rocker-org/binder/) and an [MRAN snapshot](https://mran.microsoft.com/timemachine) of July 5th 2019.
nolinenumbers: false # disable line numbering
hideLIPIcs: false # remove references to LIPIcs series (logo, DOI, ...), e.g. when preparing a pre-final version to be uploaded to arXiv or another public repository
# appendix _after_ the bibliography
#appendix: |
output:
  # xelatex needed for 'thin space' Unicode character (see authorrunning);
  bookdown::pdf_book:
    base_format: rticles::lipics_article
    latex_engine: xelatex
    keep_tex: TRUE
---

# Introduction

A large proportion of GIScience research today uses software to analyse data on computers.
This makes a considerable share of the articles published in the context of the GIScience conference series[^giscience] fall into the categories of data science or computational research.
Thereby these articles are exposed to challenges of transparency and reproducibility in the sense of the Claerbout/Donoho/Peng terminology \cite{barba_terminologies_2018}, where reproduction means a recreation of the same results using the same input data and workflow as the original authors.
In previous work \cite{nust_reproducible_2018} we assessed the reproducibility of full and short papers of the AGILE conference series[^agile], a community conference closely related to GIScience. 
Using a rubric for reproducible research, we found that the majority of articles did not provide sufficient information for a reviewer to evaluate code and data and attempt a reproduction, and neither enough material for readers to reuse or extend the workflows.
This is corroborated by research in related disciplines such as quantitative geography \cite{brunsdon_quantitative_2016}, qualitative GIS \cite{muenchow_reviewing_2019}, geoscience \cite{konkol_computational_2018}, and computer science [TODO REF].
The problems identified in these related research areas are directly transferable to GIScience, which operates at the intersections of aforementioned fields \cite{Goodchild1992}.
In any case, observations on lack of reproducibility in all scientific fields are contrasted by clear advantages and benefits  of open and reproducible research both for individuals and for academia as a whole (cf. for example \cite{donoho_invitation_2010,markowetz_five_2015,kray_reproducible_2019,Colavizza2020}).

We have conducted a simple text analysis of the proceedings[^textanalysis] to evaluate the relevance of computational methods in GIScience conference papers.
The analysis finds wordstems related to reproducibility: generic words, e.g., "data", "software", or "process", specific platforms, e.g., "GitHub", and concrete terms, e.g., words starting with "reproduc".
The take away message from the text analysis that algorithms, processing, and data play a large role in GIScience publications, but only little mentioning of code repositories or reproduction material could be identified.
Therefore, a more detailed manual assessment of the reproducibility of these publications is necessary.
The AGILE and GIScience conference series are conference series that share a common discipline, and have significant overlap in audience.
<!-- TODO: quantify overlap -->
Nevertheless, based on our own experience with these two events, we postulate that there are also significant differences in terms of topic focus and methods applied, that preclude the simplistic assumption that the findings for AGILE apply to GIScience as well, and instead warrant an investigation. 

The main contribution of this work addresses two objectives:
First, to apply the assessment procedure used for AGILE conference papers (presented in the next section) to the papers of the GIScience conference, to check whether it is generic enough to allow replication with a different dataset.
This transfer solidifies the methodology and yields important findings for the discussion of reproducibility within the GIScience conference community and the broader GIScience discipline at large. Second, to broaden our knowledge base about reproducibility in GIScience in general, and learn more about the situation in the GIScience conference series.
Only then can a fruitful discussion take place on ways to improve reproducibility (if necessary) for the GIScience conference series, and whether the recent steps taken at AGILE \cite{reproducible_agile} could be an inspiration for GIScience conferences as well.
We discuss these findings and present our conclusions in the final two sections.

[^giscience]: [https://www.giscience.org/](https://www.giscience.org/)
[^agile]: [https://agile-online.org/conference](https://agile-online.org/conference)
[^textanalysis]: The full text analysis results is available in this paper's repository: [`giscience-historic-text-analysis.Rmd`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/giscience-historic-text-analysis.Rmd) contains the analysis code and the result files are [`text_analysis_topwordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_topwordstems.csv) and [`text_analysis_keywordstems.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/text_analysis_keywordstems.csv).

# Related work [Daniel, then Barbara + Carlos]

- https://www.nature.com/articles/sdata201930
- “Computational Reproducibility in The Wildlife Society's Flagship Journals” https://doi.org/10.1002/jwmg.21855 (also use a kind of rating scheme)
- Opening practice: supporting reproducibility and critical spatial data science > https://link.springer.com/article/10.1007/s10109-020-00334-2
- Reproducibility and replicability: opportunities and challenges for geospatial research > https://www.tandfonline.com/doi/abs/10.1080/13658816.2020.1802032
<!-- TODO: ... -->

Reproducibility and replicability: opportunities and challenges for geospatial research

# Reproducibility Assessment

## Criteria

The overall approach to assessing the reproducibility of GIScience papers is similar to the assessment of AGILE papers \cite{nust_reproducible_2018}:
Two members of the author team reviewed each GIScience paper along three main criteria, assigning one of four reproducibility levels to each criterion.
The three assessment criteria are _Input Data_, _Methods_, and _Results_. 
_Input Data_ comprises all datasets that the computational analysis uses.
_Methods_ encompasses the entire computational analysis that generates the results.
Since _Methods_ is difficult to evaluate as a whole, we split this criterion into three subcategories:
_Preprocessing_ includes the steps to prepare the _Input Data_ before main analysis;
_Methods, Analysis, Processing_ is the actual analysis; 
_Computational Environment_ addresses the description of the hard- and software.
Finally, the criterion _Results_ refers to the output of analysis, for example, figures, tables, and numbers. 
For each of these (sub)categories, we assigned one of four levels:
_(Level 0) Unavailable_ means that it is not possible to access the paper’s data, methods, or results, and that it is impossible to recreate them based on the description in the paper.
_(Level 1) Documented_ indicates that the paper still does not provide access to datasets, methods, or results, but that there is sufficient description to recreate them closely enough for an evaluation, yet often a recreation is unlikely due to huge efforts needed.
_(Level 2) Available_ is assigned if the paper provides direct access to the materials (e.g., through a personal or institutional website), but not in the form of an open and permanent identifier, such as a DOI. 
The gold standard, _(Level 3) Available and open_, requires open and permanent access to the materials (e.g., through public online repositories).

## Paper Corpus

```{r load_data, echo=FALSE, message=FALSE, warning=FALSE}
library("here")

assessment_file <- here::here("results/paper_assessment.csv")

category_levels <- c("0", "1", "2", "3")
categoryColumns <- c("input data", 
                     "preprocessing",
                     "method/analysis/processing",
                     "computational environment",
                     "results")

paper_evaluation <- readr::read_csv(assessment_file, 
    col_types = readr::cols(
      `conceptual paper` = readr::col_logical(),
      `computational environment` = readr::col_factor(levels = category_levels),
      `input data` = readr::col_factor(levels = category_levels),
      `method/analysis/processing` = readr::col_factor(levels = category_levels),
      preprocessing = readr::col_factor(levels = category_levels),
      results = readr::col_factor(levels = category_levels)
      ),
    na = "NA")
```

```{r count_papers, echo=FALSE, message=FALSE, warning=FALSE}
library("dplyr")

count_papers <- nrow(paper_evaluation)

count_conceptual <- nrow(paper_evaluation %>% 
                           dplyr::filter(`conceptual paper` == TRUE))
                           
count_mixed <- nrow(paper_evaluation %>% 
                      dplyr::filter(is.na(`input data`) 
                             | is.na(preprocessing) 
                             | is.na(`method/analysis/processing`) 
                             | is.na(`computational environment`) 
                             | is.na(results)))

paper_evaluation_wo_conceptual <- filter(paper_evaluation, `conceptual paper` == FALSE)

count_n <- nrow(paper_evaluation_wo_conceptual)
```

## Process [Barbara]

Process similar to AGILE etc.

- Since there is no pre-selection, we looked at all GIScience papers, starting at 2018 and then iterating backwards until 20XX
- 5 assessors, all of which participated in the AGILE assessment
- We seem to have gotten stricter and applied “0” in cases where we previously might have used “NA” > reevaluation of the preprocessing category resulted in a few 0s being switched to NAs instead
- Should we write down how we _now_ understand _pre_processing?
- If the authors use an extra section for some steps of the workflow, that is a hint at preprocessing
- In “input data”, we clarified that “available” must have been the case once, but might not be now, e.g., if the given URL of an official site has changed; we gave the authors the benefit of the doubt and assume the data was accessible in the months after the conference/paper publication; we don't give it an arbitrary number, and these papers do not get a 3 (which includes permanency)
- Anecdotally, the papers seem to have more formulas
- How did we handle human subject experiments > needed to update the common understanding of the categories
- Note: this work is qualitative, and therefore not reproducible, so we try to be as transparent as possible
- human-studies were rated as 1 if sufficiently documented (though "original sources" or recreatable in the strict sense does not fit)
- mention that "11111" is a "classical" paper that is fully understandable?
- using a PDF reader with multiple highlights can be useful, words e.g. "data, software, code, download, contribution, script, workflow"
- for simulation data, we used "data NA" and considered these parameters a part of "analysis" > can be clarified in assessment examples
- need clear guidance on human subject studies and surveys, see 152014 and 162014, just to be on the same page


## Results

### Quantitative results

- `r count_papers` papers from the conferences in `r paste(sort(unique(paper_evaluation$year)), collapse = ", ")` were assessed.
- `r count_conceptual` papers across all years were identified as conceptual papers[^conceptual] and were not included in the corpus. This results in `paste0(round(count_conceptual / count_papers * 100, 0), "%")` out the corpus. For comparison, AGILE had 5 conceptual out of 32 (`paste0(round(5 / 32 * 100, 0), "%"`).
- Anecdotally, the number of conceptual papers in GIScience conferences has steadily decreased over the analised years. `r paper_evaluation %>% filter(``conceptual paper`` == TRUE) %>% group_by(year) %>% summarise(n= n())`, resulting in no conceptual papers in the last year's proceedings (`max(unique(paper_evaluation$year))`).
- In total, `r count_n` are assessed according to the reproducibility criteria. 

### Assessment of reproducibility levels
 
Table&nbsp;\@ref(tab:summary-evaldata) shows aggregated values for the assessed reproducibility levels. **NOTE CARLOS: I would remove conceptual papers (8) from the table. This will impact only on the totals of NA's, being 0 for all but "preproc." with 13 NA's. I've updated the code accordingly**.
    
- `r paper_evaluation_wo_conceptual %>% filter(is.na(preprocessing)) %>% count() %>% .$n` papers are not applicable for preprocessing criterion. This can be explained because the boundary between the preprocessing criterion and the methods/analysis/processing criterion is not always clear in a given paper and is therefore subject to the reader's interpretation. This does not mean that the papers that did not qualify for the preprocessing criteron are not reproducible at all. Simply, in these cases, either no preprocessing is required or has been integrated into the main analysis. Obviously, the ability to repeudede a paper is seriously limited if data preprocessing is needed but is not indicated in the paper. 
- If we look at the median values of the five criteria, a typical GIScience paper scores `11101`. This rubric translates in practical terms into a paper that's sufficiently documented to claim that reproduction could be attempted. Nevertheless, an acceptable level of pre-reproducibility does not guaranttee actual reproduction. If we had to reproduce such a typical paper, we would require a lot of effort, technical skills and time to both gather, recreate, and/or analyse all the necessary resources (data, code, etc), and to recreate the specific computational environment of the paper, if the latter were possible after all since it is not specified on average (note the value "0" in `11101`).

```{r criteria_numbers, echo=FALSE, message=FALSE, warning=FALSE}

data_level_zero <- paper_evaluation_wo_conceptual %>% 
  filter(`input data` == 0) %>% 
  count() %>% .$n

data_level_two <- paper_evaluation_wo_conceptual %>% 
  filter(`input data` == 2) %>% 
  count() %>% .$n

preprocessing_included <- paper_evaluation_wo_conceptual %>% 
  filter(!is.na(preprocessing)) %>% 
  count() %>% .$n

preprocessing_level_one <- paper_evaluation_wo_conceptual %>% 
  filter(preprocessing == 1) %>% 
  count() %>% .$n

methods_and_results_eq_one <- paper_evaluation_wo_conceptual %>% 
  filter(`method/analysis/processing` == 1 & results == 1) %>% 
  count() %>% .$n
  
compenv_level_zero <- paper_evaluation_wo_conceptual %>% 
  filter(`computational environment` == 0) %>% 
  count() %>% .$n
  
```

Figure&nbsp;\@ref(fig:assessment-results) shows the distribution of the reproducibility levels for each criterion. None of the papers reach the highest level of reproducibility in any criterion. 
- Only `r data_level_two` papers reach level 2 in the data criterion, which is still the highest number of that level across all criteria. Similar to previous results \cite{nust_reproducible_2018}, the number of papers with level 0 for data is especially high (`r data_level_zero`, `r paste0(round(data_level_zero / count_n * 100, 0), "%")`), which is a heavy barrier to reproduction since input data is not only unavailable, but cannot be recreated from the information in the paper.
- Data preprocessing applies to `r preprocessing_included` publications, and the levels are generally low (0,1), being level 2 almost residual.
- Methods and results criteria show a pretty similar distribution (Figure&nbsp;\@ref(fig:assessment-results). Indeed, `r methods_and_results_eq_one` publications have level 1 in both criteria, which represents `r paste0(round(methods_and_results_eq_one / count_n * 100, 0), "%")` of the papers assessed. Note that levels are ordinal numbers that  can be compared (3 is higher than 2), but absolute differences between numbers must not be interpreted as equals. Moving one level up from 0 to 1 is not the same as from 1 to 2. While reaching 1 is fairly straightforward, jumping to level 2 means to almost have a truly reproducible paper. In this sense, most of the assessed papers fall below the minimum standard for reproduction as regards the methods and results criteria. No `0` for results shows that peer review worked as expected for these papers. In other words, the authors are concerned with making the results understandable to the reviewers, which is not always the case for the rest of the criteria. More generally, this aspect raises the question of whether peer review should stop in the absence of minimal evidence of the input data, analysis, and computational environment used in a paper. Without an explicit justification (confidential data, privacy concerns, etc), should a paper with level 0 (or NA) for the data criterion get accepted for a conference? 
- Finally, the computational environment criterion obtains the worst results. `r compenv_level_zero` (`r paste0(round(compenv_level_zero / count_n * 100, 0), "%")`) publications are level 0, which means that no clues are provided in the paper about the computing environment, tools and libraries used in the reported research and/or analysis. The computational environment criterion and the input data criterion account for a considerable number of `0s`, which clearly signals a serious impediment to reproduction.


The full assessment table in the paper repository is at [`paper_assessment.csv`](https://github.com/nuest/reproducible-research-at-giscience/blob/master/results/paper_assessment.csv) 

```{r summary-evaldata, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%'}
library("knitr")
library("kableExtra")

evaldata_numeric <- paper_evaluation_wo_conceptual %>%
  # must convert factors to numbers to calculate the mean and median
  dplyr::mutate_if(is.factor, dplyr::funs(as.integer(as.character(.))))

summaryna <- function (v) {
  if(!any(is.na(v))){
    res <- c(summary(v),"NA's"=0)
  } else{
    res <- summary(v)
  }
  return(res)
} 


# apply summary independently to format as table
summaries <- sapply(evaldata_numeric[,categoryColumns], summaryna)
exclude_values_summary <- c("1st Qu.", "3rd Qu.")
kable(subset(summaries, !(rownames(summaries) %in% exclude_values_summary)), 
      digits = 1,
      col.names = c("input data", "preproc.", "method/analysis/proc.",
                    "comp. env.", "results"),
      caption = "Statistics of reproducibility levels per criterion (rounded to one decimal place)")
```

```{r assessment-results, echo=FALSE, message=FALSE, warning=FALSE, out.width='100%', fig.cap="Barplots of reproducibility assessment results; levels range from 0 (leftmost bar) to 'not applicable' (rightmost bar)."}
# match the colours to time series plot below
colours <- RColorBrewer::brewer.pal(length(categoryColumns), "Set1")
level_names <- c("0", "1", "2", "3", "NA")

criteriaBarplot = function(data, main, colour) {
  barplot(table(data, useNA = "always"), 
          main = main,
          xlab = "Level",
          ylim = c(0,nrow(paper_evaluation_wo_conceptual)),
          names.arg = level_names, col = colours[colour])
}

par(mfrow = c(1,length(categoryColumns)),
    cex = 0.5,
    cex.axis = 0.9)
criteriaBarplot(paper_evaluation_wo_conceptual$`input data`,
                main = "Input data", colour = 1)
criteriaBarplot(paper_evaluation_wo_conceptual$`preprocessing`, 
                main = "Preprocessing", colour = 2)
criteriaBarplot(paper_evaluation_wo_conceptual$`method/analysis/processing`,
                main = "Methods/Analysis/\nProcessing", colour = 3)
criteriaBarplot(paper_evaluation_wo_conceptual$`computational environment`,
                main = "Computational\nEnvironment", colour = 4)
criteriaBarplot(paper_evaluation_wo_conceptual$results,
                main = "Results", colour = 5)
```

[^conceptual]: See \cite{nust_reproducible_2018} for a definition of "conceptual".

### A look on the papers with highest reproducible levels?

- Does it make sense to pay attention to papers with one or more `2`? Attempt a reproduction? 

# Discussion

- Rubric worked, but faced similar challenges (category classes not equidistant!)
- Publication years similar to assessment of AGILE papers, so we think results are comparable in the sense of what methods and tools would have been available for authors.
- comparision to AGILE?
- A recurrent issue found in the analysis is the inability to access input data (today) based on the information provided in the paper. Most of the links and pointers to datasets reported at the time of publications were either broken (e.g. inexistent resourse - 404 error, invalid URL sintax) or not available anymore (URL works but website/resource specified in the paper no loger exists). In these cases, a level of 2 in the input data criterion was well deserved at the time of publciaiton. However, when evaluating the level of reproducibility some time later (this paper), level 2 is no longer acceptable for those papers from the point of view of reproducibility since input data is not accessible anymore unless contacting directly with the authors, which is equivalent in practical terms to the statement "available upon request" (level 0). A basic principle to enable reproducibility is that accessibility to input data used in a paper should not degrade over time. Simple methods and techniques can be easily considered to maintain this principle for the ensuing years, and even decades (see Tenopir's trilogy about "data sharing'  https://journals.plos.org/plosone/search?filterJournals=PLoSONE&filterAuthors=Carol+Tenopir&page=1). Instrad of  short-lived URL like a personal website or a project website, authors should archive input data used in a paper in permanent repositories. Consecuently, footnotes with URLs are not suitable for citing data, but as a citation by itself like any other resource or paper included in the bibliography section.
- A paper with "all 1s" is a regular paper - nothing wrong with it using current standards, but that's a shortcoming of the current standard
- No 0s in the results is a very good thing (papers pass the bar of peer review)
- The overall picture matches a (troublesome) focus on results in science, and less care about publishing the full process
- The many 0s for input data are _worrysome_!
- 0s in preprocessing are less a concern, because there are some problems with the criterion
- We analyse the history, and no incentives to push these aspects in the past > practice is starting to change now

# Conclusions and outlook

In this work we investigated the reproducibility of several years of GIScience conference publications.
The paper corpus is large enough for a representative sample.
The corpus size is comparable to the one used for the AGILE study, but the corporas have different time windows.
It was never the intention of this study to rate the papers, or engage in a comparison of AGILE vs. GIScience conference quality.
We do not question that the research presented in these papers is sound and relevant, since they were accepted for publication at a reputable conference.
Instead, we investigated the papers along a single desirable quality dimension, reproducibility.
Assuming a similar high bar for reproducibility as in the earlier study, the results clearly show a lot of room for improvement, as none of the presented articles was readily reproducible.
The majority of articles provide some information, but not to the degree required to facilitate a change for more open, more transparent, and, most relevantly, reusable research.
In \cite{nust_reproducible_2018} we describe concrete recommendations for individuals and organisations to improve the reproducibility.
All these recommendations are directly transferable to the GIScience conference series, most importantly
(a) a promotion of outstanding reproducible work, e.g., with awards or badges,
(b) recognition of the efforts to achieve reproducibility, e.g., with a special track for reproducible papers, a role of reproducibility reviewer, open educational resources, and helpful author guidelines including data and software citation requirements and a specific data/software repository, and
(c) an institutional commitment to a shift in policies going beyond mere accessibility \cite{stodden_empirical_2018}.
These changes require a roadmap and clear year, say 2024, when GIScience starts to only accept computationally reproducible submissions and reproducibility is checked before acceptance of papers.
The AGILE reproducible paper guidelines \cite{agile_guidelines} and community code review systems such as CODECHECK \cite{eglen_codecheck_2019} are open and "ready to use", but also can be adopted for suitable reproducibility practices for the GIScience conferences.
The discourse around these recommendations would be beneficial for all members of the community, whether their work is more towards the conceptual, the computational, or the applied direction of GIScience.
An survey for authors, as conducted for AGILE \cite{nust_reproducible_2018}, can help to identify specific hindering circumstances for and special requirements of the GIScience conference community and is useful future work.
